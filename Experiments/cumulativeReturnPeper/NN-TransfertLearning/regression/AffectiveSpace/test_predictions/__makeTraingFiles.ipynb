{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/simone/Desktop/NLFF/Experiments/cumulativeReturnPeper')\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "\n",
    "from technicalSignals import Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=['AAPL','AMZN','GOOGL','MSFT','FB','INTC','CSCO','CMCSA','NVDA','NFLX']\n",
    "\n",
    "TREND_WINDOWs = [(1,2),(1,8),(1,29),(1,36),(1,50)]\n",
    "kind_of_dataset = 'AffectiveSpace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    def __init__(self):\n",
    "        X_raw = None\n",
    "        Y_raw = None\n",
    "        Y = None\n",
    "        X = None\n",
    "        cum_ret_test = None\n",
    "    \n",
    "    def load_dataset(self, ticker, kind, technicalFeatures=False):\n",
    "        types = {'Summary': '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentFullAggregatedHourly/',\n",
    "            'AffectiveSpace': '/home/simone/Desktop/NLFF/AffectiveSpace/Aggregated_AffectSummary_dataset/',\n",
    "            'Title': '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentTitleAggregatedHourly/',\n",
    "            'Senticnet':''}\n",
    "        news =  pd.read_csv(types[kind]+ticker+'.csv')\n",
    "        price = pd.read_csv('/home/simone/Desktop/NLFF/indexes/indexes'+ticker+'.csv')\n",
    "        price = price.rename(index=str, columns={\"date\": \"DATE\"})\n",
    "        news = news.rename(index=str, columns={\"initTime\": \"DATE\"})\n",
    "        news = news.drop(['Unnamed: 0'], axis=1)\n",
    "        news['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in news['DATE']]\n",
    "        # This datased is already GMT+0\n",
    "        price['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in price['DATE']]\n",
    "        if(technicalFeatures):\n",
    "            price['mom_30'] = Indicators.momentum(price, 30)\n",
    "            price['mom_50'] = Indicators.momentum(price, 50)\n",
    "            price['mom_100'] = Indicators.momentum(price, 100)\n",
    "            price['mom_150'] = Indicators.momentum(price, 150)\n",
    "            price['SMA_30'] = Indicators.SMA(price, 30)\n",
    "            price['SMA_50'] = Indicators.SMA(price, 50)\n",
    "            price['SMA_100'] = Indicators.SMA(price, 100)\n",
    "            price['SMA_150'] = Indicators.SMA(price, 150)\n",
    "            price['in_BBands'] = Indicators.inBBands(price)\n",
    "            price['eccessVolumes'] = Indicators.eccessOfVolumes(price)\n",
    "\n",
    "\n",
    "        #ALLIGNMENT\n",
    "        initDate = max(news['DATE'][0], datetime(2017, 5, 22, 0, 0, 0))\n",
    "        finalDate = min(news['DATE'][len(news)-1],datetime(2018, 6, 20, 0, 0, 0))\n",
    "        news.drop(news[news.DATE > finalDate].index, inplace=True)\n",
    "        news.drop(news[news.DATE < initDate].index, inplace=True)\n",
    "        news = news.reset_index(drop=True)\n",
    "        price.drop(price[price.DATE > finalDate].index, inplace=True)\n",
    "        price.drop(price[price.DATE < initDate].index, inplace=True)\n",
    "        price = price.reset_index(drop=True)\n",
    "        assert len(price) == len(news)\n",
    "        # FEATURES\n",
    "        sentiment = news.drop(['DATE'], axis=1)\n",
    "        X = sentiment\n",
    "        for window in [5,10,15,20,30,50]:\n",
    "            temp = sentiment.rolling(window).mean()\n",
    "            temp.columns = temp.columns +'_'+str(window)\n",
    "            X = pd.concat([X, temp],axis=1)\n",
    "        if(technicalFeatures):   \n",
    "            technical_features = ['mom_30','mom_50','mom_100','mom_150',\n",
    "                                  'SMA_30','SMA_50','SMA_100','SMA_150','in_BBands', 'eccessVolumes']\n",
    "            X = pd.concat([X, price[technical_features]],axis=1)\n",
    "\n",
    "\n",
    "        #NORMALIZATION:\n",
    "        with open('../min_max_scaler_trained_allTickers_VolumeFeature.pickle', 'rb') as handle:\n",
    "            min_max_scaler = pickle.load(handle)\n",
    "        X = np.nan_to_num(np.asarray(X, dtype=float))\n",
    "        X = np.asarray(min_max_scaler.transform(X))\n",
    "        self.X_raw = X\n",
    "        self.Y_raw = price\n",
    "\n",
    "    def get_dataset_for_trend(self, init, finish, perc_train = 0.7):\n",
    "        y = list()\n",
    "        x = list()\n",
    "        dates = list()\n",
    "        price = self.Y_raw\n",
    "        self.cum_ret_test = []\n",
    "        for i in range(abs(init),len(price)-finish):\n",
    "            cumulative_return =  (price.iloc[i+finish]['open']-price.iloc[i+init]['open'])/price.iloc[i+init]['open']\n",
    "            s =np.sign(cumulative_return)\n",
    "            y.append(0 if s==-1 else 1)\n",
    "            self.cum_ret_test.append(cumulative_return)\n",
    "            dates.append(price.iloc[i]['DATE'])\n",
    "            x.append(self.X_raw[i])\n",
    "\n",
    "        y = np.array(y)\n",
    "        x = np.array(x)\n",
    "        self.X = x\n",
    "        self.Y = y\n",
    "        nt=math.ceil(len(x)*perc_train)\n",
    "        x_tv = x[:nt]\n",
    "        y_tv = y[:nt]\n",
    "        x_test = x[nt:]\n",
    "        y_test = y[nt:]\n",
    "        self.cum_ret_test =  self.cum_ret_test[nt:]\n",
    "        dates_test = dates[nt:]\n",
    "        return (x_tv,y_tv),(x_test,y_test),dates_test\n",
    "    \n",
    "    def normalize(self, values):\n",
    "        m = min(values)\n",
    "        M = max(values)\n",
    "        values = 2*(values-m)/(M-m)-1\n",
    "        return values\n",
    "    \n",
    "    def get_dataset_for_trend_all_tickers(self, init, finish,kind, perc_train = 0.7, technicalFeatures=False):\n",
    "        x_tv_all = []\n",
    "        y_tv_all = []\n",
    "        x_test_all = []\n",
    "        y_test_all = []\n",
    "        dates_test_prev = None\n",
    "        for ticker in tickers:\n",
    "            self.load_dataset(ticker, kind, technicalFeatures)\n",
    "            (x_tv,y_tv),(x_test,y_test),dates_test = ds.get_dataset_for_trend(init, finish, perc_train = 0.7)\n",
    "            if(dates_test_prev):\n",
    "                assert dates_test == dates_test_prev #I'm not secure about this constraint but otherwise which dates I will output?\n",
    "            x_tv_all += x_tv.tolist()\n",
    "            y_tv_all += y_tv.tolist()\n",
    "            x_test_all += x_test.tolist()\n",
    "            y_test_all += y_test.tolist()\n",
    "        x_tv_all = np.asarray(x_tv_all)\n",
    "        y_tv_all = np.asarray(y_tv_all)\n",
    "        x_test_all = np.asarray(x_test_all)\n",
    "        y_test_all = np.asarray(y_test_all)\n",
    "        return (x_tv_all,y_tv_all),(x_test_all,y_test_all), dates_test\n",
    "    \n",
    "def smote(x,y):\n",
    "    X_resampled, y_resampled = SMOTE().fit_sample(x, y)\n",
    "    #print('check',sum(y_resampled)/len(y_resampled))\n",
    "    return X_resampled,y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "====================  trend:  1   2  ==================== \n",
      "\n",
      "\n",
      "AMZN\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "GOOGL\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "MSFT\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "FB\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "INTC\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "CSCO\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "CMCSA\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "NVDA\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "NFLX\n",
      "2018-02-22 20:00:00 2018-02-22 20:00:00 567 567\n",
      "2018-06-19 18:00:00 2018-06-19 18:00:00\n",
      "\n",
      "\n",
      "\n",
      "====================  trend:  1   8  ==================== \n",
      "\n",
      "\n",
      "AMZN\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "GOOGL\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "MSFT\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "FB\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "INTC\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "CSCO\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "CMCSA\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "NVDA\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "NFLX\n",
      "2018-02-22 16:00:00 2018-02-22 16:00:00 565 565\n",
      "2018-06-18 19:00:00 2018-06-18 19:00:00\n",
      "\n",
      "\n",
      "\n",
      "====================  trend:  1   29  ==================== \n",
      "\n",
      "\n",
      "AMZN\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "GOOGL\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "MSFT\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "FB\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "INTC\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "CSCO\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "CMCSA\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "NVDA\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "NFLX\n",
      "2018-02-20 15:00:00 2018-02-20 15:00:00 559 559\n",
      "2018-06-13 19:00:00 2018-06-13 19:00:00\n",
      "\n",
      "\n",
      "\n",
      "====================  trend:  1   36  ==================== \n",
      "\n",
      "\n",
      "AMZN\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "GOOGL\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "MSFT\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "FB\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "INTC\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "CSCO\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "CMCSA\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "NVDA\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "NFLX\n",
      "2018-02-16 17:00:00 2018-02-16 17:00:00 557 557\n",
      "2018-06-12 19:00:00 2018-06-12 19:00:00\n",
      "\n",
      "\n",
      "\n",
      "====================  trend:  1   50  ==================== \n",
      "\n",
      "\n",
      "AMZN\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "GOOGL\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "MSFT\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "FB\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "INTC\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "CSCO\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "CMCSA\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "NVDA\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n",
      "NFLX\n",
      "2018-02-14 21:00:00 2018-02-14 21:00:00 553 553\n",
      "2018-06-08 19:00:00 2018-06-08 19:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (init, finish) in TREND_WINDOWs:\n",
    "    print('\\n\\n\\n====================  trend: ',init,' ',finish, ' ==================== \\n\\n')\n",
    "    predictions = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        ds = DatasetManager()\n",
    "        ds.load_dataset(ticker = ticker, kind = kind_of_dataset, technicalFeatures=True)\n",
    "        (x_tv,y_tv),(x_test,y_test),dates_test = ds.get_dataset_for_trend(init, finish, perc_train = 0.7)\n",
    "        y_pred = np.loadtxt(ticker+'_'+str(init)+'_'+str(finish)+'.csv', delimiter=',')\n",
    "        assert len(dates_test) == len(y_pred)\n",
    "        if not predictions.empty:\n",
    "            print(ticker)\n",
    "            print(predictions.index[0],dates_test[0],len(predictions.index),len(dates_test))\n",
    "            print(predictions.index[-1],dates_test[-1])\n",
    "            assert list(predictions.index) == dates_test\n",
    "        predictions[ticker] = y_pred\n",
    "        predictions.index = dates_test\n",
    "    predictions.to_csv('AllTickers_'+str(init)+'_'+str(finish)+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
