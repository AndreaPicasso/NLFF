{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization,LeakyReLU\n",
    "from keras import optimizers,regularizers\n",
    "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from technicalSignals import momentum,SMA,inBBands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=['AAPL','AMZN','GOOGL','MSFT','FB','INTC','CSCO','CMCSA','NVDA','NFLX']\n",
    "TREND_WINDOWs = [(-48,0),(-35,0),(-28,0),(-7,0),(-1,0),(1,2),(1,8),(1,29),(1,36),(1,50)]\n",
    "TREND_WINDOWs = [(1,2),(1,8),(1,29),(1,36),(1,50)]\n",
    "kind_of_dataset = 'AffectiveSpace'\n",
    "NN_INPUT_DIM = 716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    def __init__(self):\n",
    "        X_raw = None\n",
    "        Y_raw = None\n",
    "        Y = None\n",
    "        X = None\n",
    "    \n",
    "    def load_dataset(self, ticker, kind, technicalFeatures=False):\n",
    "        types = {'Summary': '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentFullAggregatedHourly/',\n",
    "            'AffectiveSpace': '/home/simone/Desktop/NLFF/AffectiveSpace/Aggregated_AffectSummary_dataset/',\n",
    "            'Title': '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentTitleAggregatedHourly/',\n",
    "            'Senticnet':''}\n",
    "        news =  pd.read_csv(types[kind]+ticker+'.csv')\n",
    "        price = pd.read_csv('/home/simone/Desktop/NLFF/indexes/indexes'+ticker+'.csv')\n",
    "        price = price.rename(index=str, columns={\"date\": \"DATE\"})\n",
    "        news = news.rename(index=str, columns={\"initTime\": \"DATE\"})\n",
    "        news = news.drop(['Unnamed: 0'], axis=1)\n",
    "        news['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in news['DATE']]\n",
    "        # This datased is already GMT+0\n",
    "        price['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in price['DATE']]\n",
    "        if(technicalFeatures):\n",
    "            price['mom_30'] = momentum(price, 30)\n",
    "            price['mom_50'] = momentum(price, 50)\n",
    "            price['mom_100'] = momentum(price, 100)\n",
    "            price['mom_150'] = momentum(price, 150)\n",
    "            price['SMA_30'] = SMA(price, 30)\n",
    "            price['SMA_50'] = SMA(price, 50)\n",
    "            price['SMA_100'] = SMA(price, 100)\n",
    "            price['SMA_150'] = SMA(price, 150)\n",
    "            price['in_BBands'] = inBBands(price)\n",
    "\n",
    "        #ALLIGNMENT\n",
    "        initDate = max(news['DATE'][0], datetime(2017, 5, 22, 0, 0, 0))\n",
    "        finalDate = min(news['DATE'][len(news)-1],datetime(2018, 6, 20, 0, 0, 0))\n",
    "        news.drop(news[news.DATE > finalDate].index, inplace=True)\n",
    "        news.drop(news[news.DATE < initDate].index, inplace=True)\n",
    "        news = news.reset_index(drop=True)\n",
    "        price.drop(price[price.DATE > finalDate].index, inplace=True)\n",
    "        price.drop(price[price.DATE < initDate].index, inplace=True)\n",
    "        price = price.reset_index(drop=True)\n",
    "        assert len(price) == len(news)\n",
    "        # FEATURES\n",
    "        sentiment = news.drop(['DATE'], axis=1)\n",
    "        X = sentiment\n",
    "        for window in [5,10,15,20,30,50]:\n",
    "            temp = sentiment.rolling(window).mean()\n",
    "            temp.columns = temp.columns +'_'+str(window)\n",
    "            X = pd.concat([X, temp],axis=1)\n",
    "        if(technicalFeatures):   \n",
    "            technical_features = ['mom_30','mom_50','mom_100','mom_150','SMA_30','SMA_50','SMA_100','SMA_150','in_BBands']\n",
    "            X = pd.concat([X, price[technical_features]],axis=1)\n",
    "\n",
    "            \n",
    "        #NORMALIZATION:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        X = np.nan_to_num(np.asarray(X, dtype=float))\n",
    "        X = np.asarray(min_max_scaler.fit_transform(X))\n",
    "        self.X_raw = X\n",
    "        self.Y_raw = price\n",
    "\n",
    "    def get_dataset_for_trend(self, init, finish, perc_train = 0.7):\n",
    "        y = list()\n",
    "        x = list()\n",
    "        dates = list()\n",
    "        price = self.Y_raw\n",
    "        for i in range(abs(init),len(price)-finish):\n",
    "            cumulative_return =  (price.iloc[i+finish]['open']-price.iloc[i+init]['open'])/price.iloc[i+init]['open']\n",
    "            s =np.sign(cumulative_return)\n",
    "            y.append(0 if s==-1 else 1)\n",
    "            dates.append(price.iloc[i]['DATE'])\n",
    "            x.append(self.X_raw[i])\n",
    "\n",
    "        y = np.array(y)\n",
    "        x = np.array(x)\n",
    "        self.X = x\n",
    "        self.Y = y\n",
    "        nt=math.ceil(len(x)*perc_train)\n",
    "        x_tv = x[:nt]\n",
    "        y_tv = y[:nt]\n",
    "        x_test = x[nt:]\n",
    "        y_test = y[nt:]\n",
    "        dates_test = dates[nt:]\n",
    "        return (x_tv,y_tv),(x_test,y_test),dates_test\n",
    "    \n",
    "    def get_dataset_for_trend_all_tickers(self, init, finish,kind, perc_train = 0.7, technicalFeatures=False):\n",
    "        x_tv_all = []\n",
    "        y_tv_all = []\n",
    "        x_test_all = []\n",
    "        y_test_all = []\n",
    "        dates_test_prev = None\n",
    "        for ticker in tickers:\n",
    "            self.load_dataset(ticker, kind, technicalFeatures)\n",
    "            (x_tv,y_tv),(x_test,y_test),dates_test = ds.get_dataset_for_trend(init, finish, perc_train = 0.7)\n",
    "            if(dates_test_prev):\n",
    "                assert dates_test == dates_test_prev #I'm not secure about this constraint but otherwise which dates I will output?\n",
    "            x_tv_all += x_tv.tolist()\n",
    "            y_tv_all += y_tv.tolist()\n",
    "            x_test_all += x_test.tolist()\n",
    "            y_test_all += y_test.tolist()\n",
    "        x_tv_all = np.asarray(x_tv_all)\n",
    "        y_tv_all = np.asarray(y_tv_all)\n",
    "        x_test_all = np.asarray(x_test_all)\n",
    "        y_test_all = np.asarray(y_test_all)\n",
    "        return (x_tv_all,y_tv_all),(x_test_all,y_test_all), dates_test\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildNN(l2_regularizer,n_units,dropout,):\n",
    "    model = Sequential()  \n",
    "    model.add(Dense(n_units, input_dim=NN_INPUT_DIM, activity_regularizer=regularizers.l2(l2_regularizer))) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU()) \n",
    "    model.add(Dropout(dropout)) #According to dropout paper the probability oaver the input layer should be more than others\n",
    "    model.add(Dense(math.floor(n_units/2), activity_regularizer=regularizers.l2(l2_regularizer))) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(math.floor(n_units/4), activity_regularizer=regularizers.l2(l2_regularizer))) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU()) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def reset_weights(model):\n",
    "    session=K.get_session()\n",
    "    for layer in model.layers:\n",
    "        if(hasattr(layer,'kernel_initializer')):\n",
    "            layer.kernel.initializer.run(session=session)\n",
    "            \n",
    "def plot_hystory(history,l2,drop,n_units):\n",
    "    f, axarr = plt.subplots(2, sharex=True)\n",
    "    f.set_figheight(7)\n",
    "    f.set_figwidth(10)\n",
    "    axarr[0].plot(history.history['acc'],'g', label='accuracy_train')\n",
    "    axarr[0].plot(history.history['val_acc'],'r', label='accuracy_val')\n",
    "    axarr[1].semilogy(history.history['loss'],'g--',label='loss_train')\n",
    "    axarr[1].semilogy(history.history['val_loss'],'r--',label='loss_val')\n",
    "    #axarr[1].set_ylim([0,2])\n",
    "    axarr[1].legend()   \n",
    "    axarr[0].legend()   \n",
    "    axarr[0].set_title('l2: '+str(l2)+' drop: '+str(drop)+' n_units: '+str(n_units))   \n",
    "    plt.show() \n",
    "            \n",
    "            \n",
    "def cv(x_tv,y_tv):\n",
    "    l2_space=[0.1,0.05,0.01]\n",
    "    drop_space=[0.2,0.5,0.7]\n",
    "    n_unit_space=[32,64,128,256]\n",
    "    best_mcc = -float(np.inf)\n",
    "    best_l2 = 0\n",
    "    best_drop = 0\n",
    "    best_n_units = 0\n",
    "    for l2 in l2_space:\n",
    "        for drop in drop_space:\n",
    "            for n_units in n_unit_space:\n",
    "                trainpoint=math.floor(len(x_tv)*0.70)\n",
    "                dimval=math.floor(len(x_tv)*0.10)\n",
    "                endval=trainpoint+dimval\n",
    "                #Cross validation\n",
    "                cvMCC = 0\n",
    "                n_cv = 3\n",
    "                nn_model = buildNN(l2_regularizer=l2,n_units=n_units,dropout=drop)\n",
    "                for i in range(0,n_cv):\n",
    "                    x_train=x_tv[0:trainpoint]\n",
    "                    y_train=y_tv[0:trainpoint]\n",
    "                    x_val=x_tv[trainpoint:endval]\n",
    "                    y_val=y_tv[trainpoint:endval]\n",
    "                    trainpoint=trainpoint+dimval\n",
    "                    endval=endval+dimval\n",
    "                    history = nn_model.fit(x_train, y_train, epochs = 400,batch_size =256, verbose=0, \n",
    "                                           validation_data=(x_val, y_val),shuffle=True)\n",
    "                    y_pred = nn_model.predict(x_val, batch_size=128, verbose=0)\n",
    "                    y_pred = [1 if y>0 else 0 for y in y_pred]\n",
    "                    confmatrix = confusion_matrix(y_val, y_pred)\n",
    "                    tn, fp, fn, tp = confmatrix.ravel()\n",
    "                    denom = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "                    mcc = 0 if denom== 0 else (tp*tn -fp*fn)/sqrt(denom)\n",
    "                    cvMCC += mcc/n_cv\n",
    "                    reset_weights(nn_model)\n",
    "                if(cvMCC > best_mcc):\n",
    "                    best_mcc = cvMCC\n",
    "                    best_l2 = l2\n",
    "                    best_drop = drop\n",
    "                    best_n_units = n_units\n",
    "                plot_hystory(history,l2,drop,n_units)  \n",
    "                \n",
    "    return (best_l2,best_drop,best_n_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (init, finish) in TREND_WINDOWs:\n",
    "    print('\\n\\n\\n====================  trend: ',init,' ',finish, ' ==================== \\n\\n')\n",
    "    ds = DatasetManager()\n",
    "    (x_tv,y_tv),(x_test,y_test),dates_test = ds.get_dataset_for_trend_all_tickers(init,finish,kind_of_dataset,perc_train=0.7,technicalFeatures=True)\n",
    "    (best_l2,best_drop,best_n_units) = cv(x_tv,y_tv)\n",
    "    nn_model = buildNN(l2_regularizer=best_l2,n_units=best_n_units,dropout=best_drop)\n",
    "    model_json = nn_model.to_json()\n",
    "    with open(\"nn_model_\"+str(init)+\"_\"+str(finish)+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    history = nn_model.fit(x_tv, y_tv, epochs = 400,batch_size = 256, verbose=0, validation_data=(x_test, y_test),shuffle=True)\n",
    "    nn_model.save_weights(\"nn_model_pretrained_weights_all_tickers_\"+str(init)+\"_\"+str(finish)+\".h5\")\n",
    "    y_pred = nn_model.predict(x_test, batch_size=256, verbose=0)\n",
    "    print('==== Test ===')\n",
    "    plot_hystory(history,best_l2,best_drop,best_n_units)  \n",
    "    y_pred = [1 if y>0 else 0 for y in y_pred]\n",
    "    acc = sum([1 if y_pred[i]*y_test[i]>0 else 0 for i in range(len(y_pred))])/len(y_pred)\n",
    "    confmatrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confmatrix.ravel()\n",
    "    denom = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "    mcc = 0 if denom== 0 else (tp*tn -fp*fn)/sqrt(denom)\n",
    "    print('test Acc: ',acc,' test MCC: ',mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
