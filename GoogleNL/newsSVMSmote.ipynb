{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import geometric_mean_score\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "np.random.seed(13)\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from __future__ import division\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(x,y):\n",
    "    X_resampled, y_resampled = SMOTE().fit_sample(x, y)\n",
    "    #print('check',sum(y_resampled)/len(y_resampled))\n",
    "    return X_resampled,y_resampled\n",
    "def balance(x,y):\n",
    "    posindex=np.where( y == 1 )\n",
    "    negindex=np.where( y == 0 )\n",
    "    xt=[]\n",
    "    yt=[]\n",
    "    yindex=[]\n",
    "    nindex=min(len(posindex[0]),len(negindex[0]))\n",
    "\n",
    "    #for i in range(1,nindex):\n",
    "    yt=np.concatenate((y[posindex[0][0:nindex]],y[negindex[0][0:nindex]]))\n",
    "    xt=np.concatenate((x[posindex[0][0:nindex]],x[negindex[0][0:nindex]]))\n",
    "    \n",
    "    return xt,yt\n",
    "\n",
    "def balanceup(x,y):\n",
    "    posindex=np.where( y == 1 )\n",
    "    negindex=np.where( y == 0 )\n",
    "    xt=[]\n",
    "    yt=[]\n",
    "    yindex=[]\n",
    "    \n",
    "    if(len(posindex[0])!=0 and len(negindex[0])!=0):\n",
    "       \n",
    "        nindex=max(len(posindex[0]),len(negindex[0]))\n",
    "        mini=min(len(posindex[0]),len(negindex[0]))\n",
    "        diff=nindex-mini\n",
    "        u=0\n",
    "        for i in range(0,mini):\n",
    "            yt.append(y[posindex[0][i]])\n",
    "            yt.append(y[negindex[0][i]])\n",
    "            xt.append(x[posindex[0][i]])\n",
    "            xt.append(x[negindex[0][i]])\n",
    "        #print('first',sum(yt)/len(yt)) \n",
    "        if(len(posindex[0])>len(negindex[0])):\n",
    "            toextract=negindex\n",
    "            enter=posindex\n",
    "        else:\n",
    "            toextract=posindex\n",
    "            enter=negindex\n",
    "        if(diff!=0 and len(toextract[0])!=0):\n",
    "            for i in range(0,diff):\n",
    "                r=np.random.randint(0,len(toextract))\n",
    "                yt.append(y[toextract[0][r]])\n",
    "                xt.append(x[toextract[0][r]])\n",
    "                yt.append(y[enter[0][mini+i]])\n",
    "                xt.append(x[enter[0][mini+i]])\n",
    "    else:\n",
    "        #print('Unbalance')\n",
    "        u=1\n",
    "        xt=x\n",
    "        yt=y\n",
    "    #print(sum(yt)/len(yt))              \n",
    "    return xt,yt,u\n",
    "def expercentage(y,prediction,percentage,t):\n",
    "    tot=0\n",
    "    falsetotpos=0\n",
    "    falsetotneg=0\n",
    "    for i in range(0,len(percentage)-1):\n",
    "        if(prediction[i]==1):\n",
    "            tot=tot+percentage[i]\n",
    "        if(prediction[i]==0):\n",
    "            tot=tot-percentage[i]\n",
    "            \n",
    "        falsetotpos=falsetotpos+percentage[i]\n",
    "        falsetotneg=falsetotneg-percentage[i]\n",
    "        i=i+t\n",
    "    return tot,falsetotneg, falsetotpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========    SVM + GOOGLE SENTIMENT   =================\n",
    "#\n",
    "# EXPERIMENT WITH TIMESTEP 1H\n",
    "# EXPERIMENT WITH TIMESTEP 15MIN googleSentPerTimestep15min.csv\n",
    "#\n",
    "\n",
    "news =  pd.read_csv('/home/simone/Desktop/NLFF/GoogleNL/googleSentPerTimestep.csv')\n",
    "price = pd.read_csv('/home/simone/Desktop/NLFF/indexes/indexesAAPL.csv')\n",
    "#price = pd.read_csv('/home/simone/Desktop/NLFF/TechnicalDatasetUpdated/AAPL.csv')\n",
    "\n",
    "price = price.rename(index=str, columns={\"date\": \"DATE\"})\n",
    "news = news.rename(index=str, columns={\"initTime\": \"DATE\"})\n",
    "\n",
    "news['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in news['DATE']]\n",
    "# This datased is already GMT+0\n",
    "price['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in price['DATE']]\n",
    "news['entSentVec'] = [ast.literal_eval(row) for row in news['entSentVec']]\n",
    "\n",
    "#ALLIGNMENT\n",
    "initDate = news['DATE'][0]\n",
    "finalDate = news['DATE'][len(news)-1]\n",
    "\n",
    "price.drop(price[price.DATE > finalDate].index, inplace=True)\n",
    "price.drop(price[price.DATE < initDate].index, inplace=True)\n",
    "price = price.reset_index(drop=True)\n",
    "\n",
    "for data in news['DATE']:\n",
    "    if data not in price['DATE'].tolist():\n",
    "        print(data)\n",
    "\n",
    "assert len(price) == len(news)\n",
    "\n",
    "\n",
    "\n",
    "sentiment = np.asarray(news['entSentVec'].tolist())\n",
    "sentiment = pd.DataFrame(data=sentiment)\n",
    "meanvector = pd.concat([sentiment, sentiment.rolling(5).mean()],axis=1)\n",
    "meanvector = pd.concat([meanvector, sentiment.rolling(10).mean()],axis=1)\n",
    "meanvector = pd.concat([meanvector, sentiment.rolling(15).mean()],axis=1)\n",
    "meanvector = pd.concat([meanvector, sentiment.rolling(20).mean()],axis=1)\n",
    "\n",
    "\n",
    "print(meanvector.shape)\n",
    "\n",
    "# prices=list()\n",
    "# sentiment=list()\n",
    "# for file in files:\n",
    "#     sentimentVector =  pd.read_csv('/home/andrea/Desktop/NLFF/intrinioDatasetUpdated/Sentiment/'+file+'.csv')\n",
    "#     price= pd.read_csv('/home/andrea/Desktop/NLFF/TechnicalDatasetUpdated/'+file+'.csv')\n",
    "#     #alignment of data\n",
    "#     maxdata=max(sentimentVector['initTime'])\n",
    "#     mindata=min(sentimentVector['initTime'])\n",
    "#     price['Unnamed: 0'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in price['Unnamed: 0']]\n",
    "#     price['Unnamed: 0'] = [row-timedelta(hours=8) for row in price['Unnamed: 0']]\n",
    "#     price=price[price['Unnamed: 0']>=mindata]\n",
    "#     price=price[price['Unnamed: 0']<=maxdata]\n",
    "#     print(sentimentVector[0:10])\n",
    "#     print(price[0:10])\n",
    "#     sentimentVector=sentimentVector.drop(['Unnamed: 0', 'initTime' ], axis=1)\n",
    "#     meanvector=np.concatenate((sentimentVector,pd.rolling_mean(sentimentVector,2)),axis=1)\n",
    "#     meanvector=np.concatenate((meanvector,pd.rolling_mean(sentimentVector,4)),axis=1)\n",
    "#     meanvector=np.concatenate((meanvector,pd.rolling_mean(sentimentVector,6)),axis=1)\n",
    "#     meanvector=np.concatenate((meanvector,pd.rolling_mean(sentimentVector,8)),axis=1)\n",
    "#     price=price.drop(['Unnamed: 0'],axis=1)\n",
    "#     #['macd','macds', 'boll_ub', 'boll_lb','rsi_6','rsi_12','vr_6_sma','wr_10','wr_6''close_10_sma''tr''middle_{}_sma'\n",
    "#     #df['dma'] = df['close_10_sma'] - df['close_50_sma'] pdi'] = cls._get_pdi(df, 14)df['mdi'] = cls._get_mdi(df, 14)\n",
    "#     #df['dx'] = cls._get_dx(df, 14) df['adx'] = df['dx_6_ema'] df['adxr'] = df['adx_6_ema']]\n",
    "#     #\n",
    "#     sentiment.append(meanvector)\n",
    "#     prices.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-968fbe5cbc4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# This datased is already GMT+0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#ALLIGNMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'"
     ]
    }
   ],
   "source": [
    "# ==========  SVM + LOUGRAM DICTIONARY /!\\ ALTERNATIVO ALLA SEZIONE PRECEDENTE  =================\n",
    "#\n",
    "# EXPERIMENT WITH TIMESTEP 1H\n",
    "# EXPERIMENT WITH TIMESTEP 15MIN googleSentPerTimestep15min.csv\n",
    "#\n",
    "\n",
    "news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentFullAggregatedHourly/AAPL.csv')\n",
    "price = pd.read_csv('/home/simone/Desktop/NLFF/indexes/indexesAAPL.csv')\n",
    "#price = pd.read_csv('/home/simone/Desktop/NLFF/TechnicalDatasetUpdated/AAPL.csv')\n",
    "\n",
    "price = price.rename(index=str, columns={\"date\": \"DATE\"})\n",
    "news = news.rename(index=str, columns={\"initTime\": \"DATE\"})\n",
    "\n",
    "news['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in news['DATE']]\n",
    "# This datased is already GMT+0\n",
    "price['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in price['DATE']]\n",
    "\n",
    "#ALLIGNMENT\n",
    "initDate = datetime(2018, 1, 2, 0, 0, 0)\n",
    "finalDate = datetime(2018, 3, 31, 0, 0, 0)\n",
    "\n",
    "news.drop(news[news.DATE > finalDate].index, inplace=True)\n",
    "news.drop(news[news.DATE < initDate].index, inplace=True)\n",
    "news = news.reset_index(drop=True)\n",
    "price.drop(price[price.DATE > finalDate].index, inplace=True)\n",
    "price.drop(price[price.DATE < initDate].index, inplace=True)\n",
    "price = price.reset_index(drop=True)\n",
    "\n",
    "for data in news['DATE']:\n",
    "    if data not in price['DATE'].tolist():\n",
    "        print(data)\n",
    "\n",
    "assert len(price) == len(news)\n",
    "\n",
    "\n",
    "sentiment = news.drop(['Unnamed: 0', 'DATE'], axis=1)\n",
    "\n",
    "meanvector = pd.concat([sentiment, sentiment.rolling(5).mean()],axis=1)\n",
    "meanvector = pd.concat([meanvector, sentiment.rolling(10).mean()],axis=1)\n",
    "meanvector = pd.concat([meanvector, sentiment.rolling(15).mean()],axis=1)\n",
    "meanvector = pd.concat([meanvector, sentiment.rolling(20).mean()],axis=1)\n",
    "\n",
    "\n",
    "print(meanvector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "chartlist=[]\n",
    "gainlist=[]\n",
    "\n",
    "   \n",
    "\n",
    "senttemp=np.nan_to_num(np.asarray(meanvector, dtype=float))\n",
    "senttemp=normalize(senttemp,axis=0,norm='max')\n",
    "\n",
    "percpostotal=[]\n",
    "percnegtotal=[]\n",
    "TREND_WINDOW = 4\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "percentage=[]\n",
    "\n",
    "yvolatility=[]\n",
    "#print('============================================================')\n",
    "#print('Working on window:',t)\n",
    "#print(len(xtemp))\n",
    "##QUI C E L'UNICO APPUNTO GUARDA SE CON +1 CAMBIA\n",
    "up = 0\n",
    "down = 0\n",
    "for i in range(0,len(price)-TREND_WINDOW-1):\n",
    "    s=np.sign(price.iloc[i+TREND_WINDOW+1]['close']-price.iloc[i+1]['open'])\n",
    "    percentage.append((100*(price.iloc[i+TREND_WINDOW+1]['close']-price.iloc[i+1]['open']))/price.iloc[i+1]['open']) \n",
    "    if(s==-1):\n",
    "        y.append(0)\n",
    "        down +=1\n",
    "    else:\n",
    "        y.append(1)\n",
    "        up +=1\n",
    "    yvolatility.append((100*abs(price.iloc[i+TREND_WINDOW+1]['close']-price.iloc[i+1]['open']))/price.iloc[i+1]['open'])    \n",
    "    x.append(senttemp[i])\n",
    "    \n",
    "print('UP: '+str(up)+'\\t DOWN: '+str(down))\n",
    "\n",
    "y=np.array(y)\n",
    "x=np.array(x)\n",
    "percentage=np.array(percentage)\n",
    "permindex=range(0,len(x))\n",
    "#permindex=np.random.permutation(permindex)\n",
    "train=0.8\n",
    "nt=math.ceil(len(x)*train)\n",
    "trainvalindex=permindex[0:nt]\n",
    "testindex=permindex[nt:]\n",
    "\n",
    "yvolatility=np.array(yvolatility)\n",
    "x_tv=[]\n",
    "y_tv=[]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "x_tv=x[trainvalindex]\n",
    "y_tv=y[trainvalindex]\n",
    "x_test=x[testindex]\n",
    "y_test=y[testindex]\n",
    "print('UP TRAIN-VAL: '+str(sum(y_tv)/len(y_tv)*100)+'%')\n",
    "print('UP TEST: '+str(sum(y_test)/len(y_test)*100)+'%')\n",
    "\n",
    "yvolatilitytest=yvolatility[testindex]\n",
    "#create structure for percentile valuation\n",
    "distribution=[]\n",
    "\n",
    "step=(max(yvolatilitytest)-min(yvolatilitytest))/5\n",
    "for v in range(0,5):\n",
    "    pindexes=[]\n",
    "    for r in range(0,len(y_test)):\n",
    "        if(yvolatilitytest[r]<min(yvolatilitytest)+step+step*v and yvolatilitytest[r]>min(yvolatilitytest)+step*v):\n",
    "            pindexes.append(r)\n",
    "    distribution.append(pindexes)\n",
    "\n",
    "\n",
    "cspace=np.logspace(-4,4,10)\n",
    "bestsvm=None\n",
    "maxacc=0\n",
    "cvacc=0\n",
    "maxc=0\n",
    "\n",
    "totu=0\n",
    "for c in cspace:\n",
    "#print()\n",
    "\n",
    "    cvacclist=[]\n",
    "    #faccio cross validation\n",
    "    #start with 40% as train and 10% for validation and then i move in percentege\n",
    "    # 0-40 40-50\n",
    "    # 0-50 50-60\n",
    "    # 0-60 60-70 \n",
    "    #etc test set is completely external i do in some way error extimation changin the ticker\n",
    "    trainpoint=math.floor(len(x_tv)*0.40)\n",
    "    dimval=math.floor(trainpoint*0.25)\n",
    "    endval=trainpoint+dimval\n",
    "\n",
    "    for i in range(0,6):\n",
    "        x_train=x_tv[0:trainpoint]\n",
    "        y_train=y_tv[0:trainpoint]\n",
    "        x_val=x_tv[trainpoint:endval]\n",
    "        y_val=y_tv[trainpoint:endval]\n",
    "\n",
    "        trainpoint=trainpoint+dimval\n",
    "        endval=endval+dimval\n",
    "        p=sum(y_train)/(len(y_train)-sum(y_train))\n",
    "        rbf_svm=svm.SVC(kernel='linear',C=c)\n",
    "        \n",
    "        x_train,y_train=smote(x_train,y_train)\n",
    "        rbf_svm.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "        if(sum(y_val)+6<len(x_val) and sum(y_val)>6):\n",
    "            x_val,y_val=smote(x_val,y_val)\n",
    "        else:\n",
    "            totu=totu+1\n",
    "        prediction=rbf_svm.predict(x_val)\n",
    "\n",
    "        cvacclist.append(rbf_svm.score(x_val,y_val))\n",
    "\n",
    "    cvacc=sum(cvacclist)/len(cvacclist)\n",
    "    if(cvacc>maxacc):\n",
    "        maxacc=cvacc\n",
    "        maxc=c\n",
    "\n",
    "\n",
    "\n",
    "p=sum(y_tv)/(len(y_tv)-sum(y_tv))\n",
    "rbf_svm=svm.SVC(kernel='linear',C=maxc)\n",
    "x_tv,y_tv=smote(x_tv,y_tv)\n",
    "rbf_svm.fit(x_tv,y_tv)\n",
    "prediction=rbf_svm.predict(x_test)\n",
    "volr=[]\n",
    "voln=[]\n",
    "for j in range(0,len(x_test)):\n",
    "    if(y_test[j]==prediction[j]):\n",
    "        volr.append(yvolatilitytest[j])\n",
    "    else:\n",
    "        voln.append(yvolatilitytest[j])\n",
    "confmatrix=confusion_matrix(y_test, prediction)\n",
    "CM = f1_score(y_test, prediction)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, prediction)\n",
    "auc=metrics.auc(fpr, tpr)\n",
    "valueacc=rbf_svm.score(x_test,y_test)\n",
    "\n",
    "print('F1,score',CM)\n",
    "print('Acc-score:',valueacc)\n",
    "print('Window:',TREND_WINDOW)\n",
    "print(confmatrix)\n",
    "print('Vol pos',sum(volr)/len(volr))\n",
    "print('Vol neg',sum(voln)/len(voln))\n",
    "percnegtotal.append(sum(volr)/len(volr))\n",
    "percpostotal.append(sum(voln)/len(voln))\n",
    "#build structure to plot distribution onver percentile\n",
    "chart=np.zeros(5)\n",
    "for r in range(0,len(y_test)):\n",
    "    for v in range(0,5):\n",
    "        if(yvolatilitytest[r]<min(yvolatility)+step+step*v and yvolatilitytest[r]>min(yvolatility)+step*v):\n",
    "            chart[v]+=1\n",
    "plt.plot(chart)\n",
    "plt.title('How many samples in each bin of movements')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Percentage of gain:',(sum(volr)-sum(voln))/(len(volr)+len(voln)))\n",
    "print('Total gain:',len(y_test)*(sum(volr)-sum(voln))/(len(volr)+len(voln)))\n",
    "gainlist.append(len(y_test)*(sum(volr)-sum(voln))/(len(volr)+len(voln)))\n",
    "print('===============================================')\n",
    "#performances for each percentile\n",
    "chartscore=[]\n",
    "for g in range(0,len(distribution)):\n",
    "    print('Percentile: ',g)\n",
    "    tempscore=0\n",
    "    if(len(distribution[g])>0):\n",
    "        predictionp=rbf_svm.predict(x_test[distribution[g]])\n",
    "        print(confusion_matrix(y_test[distribution[g]], predictionp))\n",
    "        print(rbf_svm.score(x_test[distribution[g]],y_test[distribution[g]]))\n",
    "        tempscore=rbf_svm.score(x_test[distribution[g]],y_test[distribution[g]])\n",
    "    chartscore.append(tempscore)    \n",
    "chartlist.append(chartscore)\n",
    "print('===============================================')\n",
    "print('Mean gain over portfolio : ', sum(gainlist)/len(gainlist))\n",
    "#print all the accuracy varing the percentile\n",
    "for st in range(0,len(chartlist)):\n",
    "    plt.plot(chartlist[st])\n",
    "    plt.title(\"Acciuracies in various bins of movements \")\n",
    "    \n",
    "plt.show()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE, 15 min, trend = 4\n",
    "\n",
    "# UP: 845\t DOWN: 797\n",
    "# UP TRAIN-VAL: 53.50076103500761%\n",
    "# UP TEST: 43.292682926829265%\n",
    "# F1,score 0.41877256317689526\n",
    "# Acc-score: 0.5091463414634146\n",
    "# Window: 4\n",
    "# [[109  77]\n",
    "#  [ 84  58]]\n",
    "# ====\n",
    "# Percentile:  0\n",
    "# [[49 37]\n",
    "#  [43 30]]\n",
    "# 0.4968553459119497\n",
    "# Percentile:  1\n",
    "# [[45 16]\n",
    "#  [31  7]]\n",
    "# 0.5252525252525253\n",
    "# Percentile:  2\n",
    "# [[ 9 11]\n",
    "#  [ 7 11]]\n",
    "# 0.5263157894736842\n",
    "# Percentile:  3\n",
    "# [[4 8]\n",
    "#  [3 8]]\n",
    "# 0.5217391304347826\n",
    "# Percentile:  4\n",
    "# [[1 5]\n",
    "#  [0 0]]\n",
    "# 0.16666666666666666\n",
    "\n",
    "# =======\n",
    "# Mean gain over portfolio :  -1.6690018545109666\n",
    "\n",
    "\n",
    "# GOOGLE, 1h, trend = 4\n",
    "\n",
    "# UP: 219 DOWN: 203\n",
    "# UP TRAIN-VAL: 56.50887573964497%\n",
    "# UP TEST: 33.33333333333333%\n",
    "# F1,score 0.5148514851485149\n",
    "# Acc-score: 0.4166666666666667\n",
    "# Window: 4\n",
    "# [[ 9 47]\n",
    "#  [ 2 26]]\n",
    "# ===============================================\n",
    "# Percentile:  0\n",
    "# [[ 3 22]\n",
    "#  [ 1 10]]\n",
    "# 0.3611111111111111\n",
    "# Percentile:  1\n",
    "# [[ 1 16]\n",
    "#  [ 0  9]]\n",
    "# 0.38461538461538464\n",
    "# Percentile:  2\n",
    "# [[1 4]\n",
    "#  [1 3]]\n",
    "# 0.4444444444444444\n",
    "# Percentile:  3\n",
    "# [[2 4]\n",
    "#  [0 3]]\n",
    "# 0.5555555555555556\n",
    "# Percentile:  4\n",
    "# [[1 0]\n",
    "#  [0 1]]\n",
    "# 1.0\n",
    "# ===============================================\n",
    "# Mean gain over portfolio :  -1.0599238818218666\n",
    "\n",
    "\n",
    "\n",
    "# DICTIONARY, 15min, trend = 4\n",
    "\n",
    "# UP: 845\t DOWN: 797\n",
    "# UP TRAIN-VAL: 53.50076103500761%\n",
    "# UP TEST: 43.292682926829265%\n",
    "# F1,score 0.5626740947075208\n",
    "# Acc-score: 0.5213414634146342\n",
    "# Window: 4\n",
    "# [[ 70 116]\n",
    "#  [ 41 101]]\n",
    "# ===============================================\n",
    "# Percentile:  0\n",
    "# [[33 53]\n",
    "#  [23 50]]\n",
    "# 0.5220125786163522\n",
    "# Percentile:  1\n",
    "# [[24 37]\n",
    "#  [11 27]]\n",
    "# 0.5151515151515151\n",
    "# Percentile:  2\n",
    "# [[ 3 17]\n",
    "#  [ 5 13]]\n",
    "# 0.42105263157894735\n",
    "# Percentile:  3\n",
    "# [[ 7  5]\n",
    "#  [ 1 10]]\n",
    "# 0.7391304347826086\n",
    "# Percentile:  4\n",
    "# [[3 3]\n",
    "#  [0 0]]\n",
    "# 0.5\n",
    "# ===============================================\n",
    "# Mean gain over portfolio :  10.505373716586945\n",
    "\n",
    "\n",
    "\n",
    "# DICTIONARY, 1H, trend = 4\n",
    "\n",
    "# UP: 219\t DOWN: 203\n",
    "# UP TRAIN-VAL: 56.50887573964497%\n",
    "# UP TEST: 33.33333333333333%\n",
    "# F1,score 0.32558139534883723\n",
    "# Acc-score: 0.6547619047619048\n",
    "# Window: 4\n",
    "# [[48  8]\n",
    "#  [21  7]]\n",
    "# ===============================================\n",
    "# Percentile:  0\n",
    "# [[20  5]\n",
    "#  [ 8  3]]\n",
    "# 0.6388888888888888\n",
    "# Percentile:  1\n",
    "# [[15  2]\n",
    "#  [ 5  4]]\n",
    "# 0.7307692307692307\n",
    "# Percentile:  2\n",
    "# [[4 1]\n",
    "#  [4 0]]\n",
    "# 0.4444444444444444\n",
    "# Percentile:  3\n",
    "# [[6 0]\n",
    "#  [3 0]]\n",
    "# 0.6666666666666666\n",
    "# Percentile:  4\n",
    "# [[1 0]\n",
    "#  [1 0]]\n",
    "# 0.5\n",
    "# ===============================================\n",
    "# Mean gain over portfolio :  31.90151202294833\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SVR (Regression) ========================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "chartlist=[]\n",
    "gainlist=[]\n",
    "\n",
    "\n",
    "senttemp=np.nan_to_num(np.asarray(meanvector, dtype=float))\n",
    "senttemp=normalize(senttemp,axis=0,norm='max')\n",
    "\n",
    "percpostotal=[]\n",
    "percnegtotal=[]\n",
    "TREND_WINDOW = 4\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "percentage=[]\n",
    "\n",
    "yvolatility=[]\n",
    "#print('============================================================')\n",
    "#print('Working on window:',t)\n",
    "#print(len(xtemp))\n",
    "##QUI C E L'UNICO APPUNTO GUARDA SE CON +1 CAMBIA\n",
    "for i in range(0,len(price)-TREND_WINDOW-1):\n",
    "    y.append(price.iloc[i+TREND_WINDOW+1]['close']-price.iloc[i+1]['open'])\n",
    "    percentage.append((100*(price.iloc[i+TREND_WINDOW+1]['close']-price.iloc[i+1]['open']))/price.iloc[i+1]['open']) \n",
    "    yvolatility.append((100*abs(price.iloc[i+TREND_WINDOW+1]['close']-price.iloc[i+1]['open']))/price.iloc[i+1]['open'])    \n",
    "    x.append(senttemp[i])\n",
    "    \n",
    "y=np.array(y)\n",
    "x=np.array(x)\n",
    "percentage=np.array(percentage)\n",
    "permindex=range(0,len(x))\n",
    "#permindex=np.random.permutation(permindex)\n",
    "train=0.8\n",
    "nt=math.ceil(len(x)*train)\n",
    "trainvalindex=permindex[0:nt]\n",
    "testindex=permindex[nt:]\n",
    "\n",
    "yvolatility=np.array(yvolatility)\n",
    "x_tv=[]\n",
    "y_tv=[]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "x_tv=x[trainvalindex]\n",
    "y_tv=y[trainvalindex]\n",
    "x_test=x[testindex]\n",
    "y_test=y[testindex]\n",
    "\n",
    "yvolatilitytest=yvolatility[testindex]\n",
    "#create structure for percentile valuation\n",
    "distribution=[]\n",
    "\n",
    "step=(max(yvolatilitytest)-min(yvolatilitytest))/5\n",
    "for v in range(0,5):\n",
    "    pindexes=[]\n",
    "    for r in range(0,len(y_test)):\n",
    "        if(yvolatilitytest[r]<min(yvolatilitytest)+step+step*v and yvolatilitytest[r]>min(yvolatilitytest)+step*v):\n",
    "            pindexes.append(r)\n",
    "    distribution.append(pindexes)\n",
    "\n",
    "\n",
    "cspace=np.logspace(-4,4,10)\n",
    "bestsvm=None\n",
    "loss= float(\"inf\")\n",
    "cvacc=0\n",
    "best_c=0\n",
    "\n",
    "totu=0\n",
    "for c in cspace:\n",
    "#print()\n",
    "\n",
    "    cvacclist=[]\n",
    "    #faccio cross validation\n",
    "    #start with 40% as train and 10% for validation and then i move in percentege\n",
    "    # 0-40 40-50\n",
    "    # 0-50 50-60\n",
    "    # 0-60 60-70 \n",
    "    #etc test set is completely external i do in some way error extimation changin the ticker\n",
    "    trainpoint=math.floor(len(x_tv)*0.40)\n",
    "    dimval=math.floor(trainpoint*0.25)\n",
    "    endval=trainpoint+dimval\n",
    "\n",
    "    for i in range(0,6):\n",
    "        x_train=x_tv[0:trainpoint]\n",
    "        y_train=y_tv[0:trainpoint]\n",
    "        x_val=x_tv[trainpoint:endval]\n",
    "        y_val=y_tv[trainpoint:endval]\n",
    "\n",
    "        trainpoint=trainpoint+dimval\n",
    "        endval=endval+dimval\n",
    "        p=sum(y_train)/(len(y_train)-sum(y_train))\n",
    "        rbf_svm=svm.SVR(kernel='linear',C=c)\n",
    "        rbf_svm.fit(x_train,y_train)\n",
    "\n",
    "        prediction=rbf_svm.predict(x_val)\n",
    "\n",
    "        cvacclist.append(rbf_svm.score(x_val,y_val))\n",
    "\n",
    "    cvloss=sum(cvacclist)/len(cvacclist)\n",
    "    if(cvloss<loss):\n",
    "        loss=cvloss\n",
    "        best_c=c\n",
    "\n",
    "\n",
    "\n",
    "p=sum(y_tv)/(len(y_tv)-sum(y_tv))\n",
    "rbf_svm=svm.SVR(kernel='linear',C=best_c)\n",
    "rbf_svm.fit(x_tv,y_tv)\n",
    "prediction=rbf_svm.predict(x_test)\n",
    "volr=[]\n",
    "voln=[]\n",
    "for j in range(0,len(x_test)):\n",
    "    if(y_test[j]==prediction[j]):\n",
    "        volr.append(yvolatilitytest[j])\n",
    "    else:\n",
    "        voln.append(yvolatilitytest[j])\n",
    "confmatrix=confusion_matrix(np.sign(y_test), np.sign(prediction))\n",
    "CM = f1_score(np.sign(y_test), np.sign(prediction))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.sign(y_test), np.sign(prediction))\n",
    "auc=metrics.auc(fpr, tpr)\n",
    "y_pred=rbf_svm.predict(x_test)\n",
    "\n",
    "print('F1-score',CM)\n",
    "print('MSE-score:',((y_test - y_pred) ** 2).sum() )\n",
    "print('Window:',TREND_WINDOW)\n",
    "print(confmatrix)\n",
    "print('Acc:',((confmatrix[0][0]+confmatrix[1][1])/(np.sum(np.asarray(confmatrix)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test)\n",
    "plt.plot(y_pred, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
