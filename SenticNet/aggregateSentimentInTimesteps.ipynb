{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import ast\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TICKER', 'FIGI_TICKER', 'FIGI', 'TITLE', 'DATE', 'URL', 'SUMMARY', 'concepts', 'polarity', 'attention', 'pleasantness', 'aptitude', 'sensitivity', 'stemmed_polarity', 'stemmed_attention', 'stemmed_pleasantness', 'stemmed_aptitude', 'stemmed_sensitivity', 'stemmed_concepts']\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT SENTIMENT AGGREGATED BY TIMESPAN\n",
    "ticker = 'AAPL'\n",
    "mypath = '/home/simone/Desktop/NLFF/Word2Vec/conceptsDataset/summary/'\n",
    "\n",
    "tickFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "news =  pd.read_csv(mypath+str(ticker)+'.csv')\n",
    "news = news.drop(['Unnamed: 0'], axis=1)\n",
    "news['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['DATE']]\n",
    "#news['entSentVec'] = [ast.literal_eval(row) for row in news['entSentVec']]\n",
    "\n",
    "print(list(news.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-03 18:00:00\n",
      "2017-04-03 17:27:34\n",
      "Start: 2017-04-03 17:27:34(2017-04-03 18:00:00) i: 0 j: 3817\n",
      "End: 2018-06-21 23:20:00\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "timeSpan = pd.read_csv(\"/home/simone/Desktop/NLFF/indexes/indexes\"+str(ticker)+\".csv\")\n",
    "timeSpan = timeSpan['date'].tolist()\n",
    "# This dataset is already GMT+0\n",
    "timeSpan = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in timeSpan]\n",
    "\n",
    "features_names = ['polarity','attention','pleasantness','aptitude','sensitivity','stemmed_polarity','stemmed_attention','stemmed_pleasantness','stemmed_aptitude','stemmed_sensitivity']\n",
    "\n",
    "sentiment = pd.DataFrame(columns=['initTime']+features_names)\n",
    "#print(list(sentiment.columns))\n",
    "\n",
    "#Prendiamo intervallo massimo dove ci sono entrambi i dataset:\n",
    "#massimo della data iniziale tra i due\n",
    "#minimo della data finale tra i due\n",
    "initDate = max(timeSpan[0], news['DATE'][0])\n",
    "finalDate = min(timeSpan[len(timeSpan)-1], news['DATE'][len(news)-1])\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "# ALLINEAMENTO INIZIO\n",
    "while(timeSpan[j] < initDate):\n",
    "    j+=1\n",
    "init = j\n",
    "while(news['DATE'][i] <= initDate - timedelta(hours=1)):\n",
    "    i+=1\n",
    "\n",
    "print(timeSpan[j])\n",
    "print(news['DATE'][i])\n",
    "    \n",
    "print(\"Start: \"+str(initDate) +\"(\"+str(timeSpan[j])+\") i: \"+str(i)+\" j: \"+str(j))\n",
    "print(\"End: \"+str(finalDate))\n",
    "\n",
    "#ALLINEAMENTO FINE DENTRO I WHILE\n",
    "while( news['DATE'][i] < finalDate and timeSpan[j] < finalDate ):\n",
    "    initTime = timeSpan[j]\n",
    "    normal_sum = {feature_name: 0 for feature_name in features_names}    \n",
    "    num_tot_news = 0\n",
    "    while(i<len(news)-1 and timeSpan[j] > news['DATE'][i]):\n",
    "        if(i%1000 == 0):\n",
    "            print(i)\n",
    "        if not (timeSpan[j] > news['DATE'][i] and timeSpan[j-1] <= news['DATE'][i]):\n",
    "            print(\"timeSpan[\"+str(j)+\"]: \"+str(timeSpan[j])+\" news[\"+str(i)+\"] : \" +str(news['DATE'][i]) + \" timeSpan[\"+str(j-1)+\"]: \"+str(timeSpan[j-1]))\n",
    "            assert False\n",
    "#         a = news['entSentVec'][i]\n",
    "#         print(type(a))\n",
    "#         print(len(a))\n",
    "#         a = np.asarray(news['entSentVec'][i])\n",
    "#         print(type(a))\n",
    "#         print(a.shape)\n",
    "            \n",
    "        normal_sum = {f : normal_sum[f]+news[f][i] for f in features_names}        \n",
    "        num_tot_news +=1\n",
    "        i+=1\n",
    "    j+=1\n",
    "    #sentiment.loc[j] = {'initTime':initTime, 'num_news':num_tot_news}\n",
    "    sentiment.loc[j,'initTime'] = initTime\n",
    "    sentiment.loc[j,'num_news'] = num_tot_news\n",
    "    if num_tot_news>0:\n",
    "        normal_sum = {f : normal_sum[f]/num_tot_news for f in features_names}        \n",
    "    for f in normal_sum.keys():\n",
    "        sentiment.loc[j,f] = normal_sum[f]\n",
    "    \n",
    "\n",
    "\n",
    "end = j\n",
    "if(len(sentiment) != end - init):\n",
    "    print(len(sentiment))\n",
    "    print(end - init + 1)\n",
    "    assert False\n",
    "\n",
    "\n",
    "sentiment.to_csv('Aggregated_senticnet_dataset/'+str(ticker)+'_senticnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initTime</th>\n",
       "      <th>polarity</th>\n",
       "      <th>attention</th>\n",
       "      <th>pleasantness</th>\n",
       "      <th>aptitude</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>stemmed_polarity</th>\n",
       "      <th>stemmed_attention</th>\n",
       "      <th>stemmed_pleasantness</th>\n",
       "      <th>stemmed_aptitude</th>\n",
       "      <th>stemmed_sensitivity</th>\n",
       "      <th>num_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-03 18:00:00</td>\n",
       "      <td>0.255305</td>\n",
       "      <td>0.193205</td>\n",
       "      <td>0.156891</td>\n",
       "      <td>0.190928</td>\n",
       "      <td>0.0641241</td>\n",
       "      <td>0.330112</td>\n",
       "      <td>0.176294</td>\n",
       "      <td>0.214781</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>0.0556677</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-03 19:00:00</td>\n",
       "      <td>0.201035</td>\n",
       "      <td>0.0182974</td>\n",
       "      <td>0.141537</td>\n",
       "      <td>0.17452</td>\n",
       "      <td>-0.0313478</td>\n",
       "      <td>0.202393</td>\n",
       "      <td>0.0883699</td>\n",
       "      <td>0.119848</td>\n",
       "      <td>0.186554</td>\n",
       "      <td>-0.0391006</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-03 20:00:00</td>\n",
       "      <td>0.267883</td>\n",
       "      <td>0.117938</td>\n",
       "      <td>0.174592</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.00211711</td>\n",
       "      <td>0.109875</td>\n",
       "      <td>0.202533</td>\n",
       "      <td>0.0129044</td>\n",
       "      <td>0.0499178</td>\n",
       "      <td>-0.0818073</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-04 14:00:00</td>\n",
       "      <td>0.194742</td>\n",
       "      <td>0.0715313</td>\n",
       "      <td>0.141741</td>\n",
       "      <td>0.147521</td>\n",
       "      <td>0.0251548</td>\n",
       "      <td>0.242885</td>\n",
       "      <td>0.0295263</td>\n",
       "      <td>0.183921</td>\n",
       "      <td>0.195147</td>\n",
       "      <td>0.0205663</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-04 15:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              initTime  polarity  attention pleasantness  aptitude  \\\n",
       "0  2017-04-03 18:00:00  0.255305   0.193205     0.156891  0.190928   \n",
       "1  2017-04-03 19:00:00  0.201035  0.0182974     0.141537   0.17452   \n",
       "2  2017-04-03 20:00:00  0.267883   0.117938     0.174592  0.212881   \n",
       "3  2017-04-04 14:00:00  0.194742  0.0715313     0.141741  0.147521   \n",
       "4  2017-04-04 15:00:00         0          0            0         0   \n",
       "\n",
       "  sensitivity stemmed_polarity stemmed_attention stemmed_pleasantness  \\\n",
       "0   0.0641241         0.330112          0.176294             0.214781   \n",
       "1  -0.0313478         0.202393         0.0883699             0.119848   \n",
       "2  0.00211711         0.109875          0.202533            0.0129044   \n",
       "3   0.0251548         0.242885         0.0295263             0.183921   \n",
       "4           0                0                 0                    0   \n",
       "\n",
       "  stemmed_aptitude stemmed_sensitivity  num_news  \n",
       "0         0.309051           0.0556677       7.0  \n",
       "1         0.186554          -0.0391006       6.0  \n",
       "2        0.0499178          -0.0818073       8.0  \n",
       "3         0.195147           0.0205663      34.0  \n",
       "4                0                   0       0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = sentiment.sort_values(by=['initTime'])\n",
    "sentiment = sentiment.reset_index(drop=True)\n",
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.to_csv('Aggregated_senticnet_dataset/'+str(ticker)+'_senticnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
