{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import ast\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT SENTIMENT AGGREGATED BY TIMESPAN\n",
    "ticker = 'AAPL'\n",
    "mypath = '/home/simone/Desktop/NLFF/Word2Vec/w2vDataset/'\n",
    "\n",
    "news =  pd.read_csv(mypath+str(ticker)+'.csv')\n",
    "news = news.drop(['Unnamed: 0'], axis=1)\n",
    "news['DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['DATE']]\n",
    "#news['entSentVec'] = [ast.literal_eval(row) for row in news['entSentVec']]\n",
    "news = news.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-03 18:00:00\n",
      "2017-04-03 17:27:34\n",
      "Start: 2017-04-03 17:27:34(2017-04-03 18:00:00) i: 0 j: 3817\n",
      "End: 2018-06-21 23:20:00\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "timeSpan = pd.read_csv(\"/home/simone/Desktop/NLFF/indexes/indexes\"+str(ticker)+\".csv\")\n",
    "timeSpan = timeSpan['date'].tolist()\n",
    "# This dataset is already GMT+0\n",
    "timeSpan = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in timeSpan]\n",
    "\n",
    "#print(list(sentiment.columns))\n",
    "\n",
    "#Prendiamo intervallo massimo dove ci sono entrambi i dataset:\n",
    "#massimo della data iniziale tra i due\n",
    "#minimo della data finale tra i due\n",
    "initDate = max(timeSpan[0], news['DATE'][0])\n",
    "finalDate = min(timeSpan[len(timeSpan)-1], news['DATE'][len(news)-1])\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "# ALLINEAMENTO INIZIO\n",
    "while(timeSpan[j] < initDate):\n",
    "    j+=1\n",
    "init = j\n",
    "while(news['DATE'][i] <= initDate - timedelta(hours=1)):\n",
    "    i+=1\n",
    "\n",
    "print(timeSpan[j])\n",
    "print(news['DATE'][i])\n",
    "    \n",
    "print(\"Start: \"+str(initDate) +\"(\"+str(timeSpan[j])+\") i: \"+str(i)+\" j: \"+str(j))\n",
    "print(\"End: \"+str(finalDate))\n",
    "\n",
    "vecs = list()\n",
    "dates = list()\n",
    "nums = list()\n",
    "cols = list(news.columns)\n",
    "cols.remove('DATE')\n",
    "\n",
    "#ALLINEAMENTO FINE DENTRO I WHILE\n",
    "while( news['DATE'][i] < finalDate and timeSpan[j] < finalDate ):\n",
    "    initTime = timeSpan[j]\n",
    "    normal_sum = np.asarray([0]*300)\n",
    "    num_tot_news = 0\n",
    "    while(i<len(news)-1 and timeSpan[j] > news['DATE'][i]):\n",
    "        if(i%1000 == 0):\n",
    "            print(i)\n",
    "        if not (timeSpan[j] > news['DATE'][i] and timeSpan[j-1] <= news['DATE'][i]):\n",
    "            print(\"timeSpan[\"+str(j)+\"]: \"+str(timeSpan[j])+\" news[\"+str(i)+\"] : \" +str(news['DATE'][i]) + \" timeSpan[\"+str(j-1)+\"]: \"+str(timeSpan[j-1]))\n",
    "            assert False \n",
    "        normal_sum = normal_sum + np.asarray(news.loc[i,cols])  \n",
    "        num_tot_news +=1\n",
    "        i+=1\n",
    "    j+=1\n",
    "    #sentiment.loc[j] = {'initTime':initTime, 'num_news':num_tot_news}\n",
    "    dates.append(initTime)\n",
    "    nums.append(num_tot_news)\n",
    "    if num_tot_news > 0:\n",
    "        normal_sum = normal_sum / num_tot_news \n",
    "    vecs.append(normal_sum)\n",
    "\n",
    "    \n",
    "sentiment = pd.DataFrame(data=vecs)\n",
    "sentiment['initTime'] = dates\n",
    "sentiment['num_news'] = nums\n",
    "    \n",
    "\n",
    "\n",
    "end = j\n",
    "if(len(sentiment) != end - init):\n",
    "    print(len(sentiment))\n",
    "    print(end - init + 1)\n",
    "    assert False\n",
    "\n",
    "\n",
    "#sentiment.to_csv('Aggregated_senticnet_dataset/'+str(ticker)+'_senticnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           False\n",
       "1           False\n",
       "2           False\n",
       "3           False\n",
       "4           False\n",
       "5           False\n",
       "6           False\n",
       "7           False\n",
       "8           False\n",
       "9           False\n",
       "10          False\n",
       "11          False\n",
       "12          False\n",
       "13          False\n",
       "14          False\n",
       "15          False\n",
       "16          False\n",
       "17          False\n",
       "18          False\n",
       "19          False\n",
       "20          False\n",
       "21          False\n",
       "22          False\n",
       "23          False\n",
       "24          False\n",
       "25          False\n",
       "26          False\n",
       "27          False\n",
       "28          False\n",
       "29          False\n",
       "            ...  \n",
       "272         False\n",
       "273         False\n",
       "274         False\n",
       "275         False\n",
       "276         False\n",
       "277         False\n",
       "278         False\n",
       "279         False\n",
       "280         False\n",
       "281         False\n",
       "282         False\n",
       "283         False\n",
       "284         False\n",
       "285         False\n",
       "286         False\n",
       "287         False\n",
       "288         False\n",
       "289         False\n",
       "290         False\n",
       "291         False\n",
       "292         False\n",
       "293         False\n",
       "294         False\n",
       "295         False\n",
       "296         False\n",
       "297         False\n",
       "298         False\n",
       "299         False\n",
       "initTime    False\n",
       "num_news    False\n",
       "Length: 302, dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = sentiment.sort_values(by=['initTime'])\n",
    "sentiment = sentiment.reset_index(drop=True)\n",
    "sentiment.head()\n",
    "sentiment.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.to_csv('Aggregated_W2V_dataset/'+str(ticker)+'_w2v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
