{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading vocabulary from /home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/vocab.txt\n",
      "INFO:tensorflow:Loaded vocabulary with 930914 words.\n",
      "INFO:tensorflow:Loading embedding matrix from /home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/embeddings.npy\n",
      "INFO:tensorflow:Loaded embedding matrix with shape (930914, 620)\n",
      "INFO:tensorflow:Building model.\n",
      "WARNING:tensorflow:From /home/simone/Desktop/NLFF/model/skip_thoughts/skip_thoughts_model.py:360: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.create_global_step\n",
      "INFO:tensorflow:Loading model from checkpoint: /home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/model.ckpt-501424\n",
      "INFO:tensorflow:Restoring parameters from /home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/model.ckpt-501424\n",
      "INFO:tensorflow:Successfully loaded checkpoint: model.ckpt-501424\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os.path\n",
    "import scipy.spatial.distance as sd\n",
    "from skip_thoughts import configuration\n",
    "from skip_thoughts import encoder_manager\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import listdir\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "VOCAB_FILE = \"/home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/vocab.txt\"\n",
    "EMBEDDING_MATRIX_FILE = \"/home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/embeddings.npy\"\n",
    "CHECKPOINT_PATH = \"/home/simone/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/model.ckpt-501424\"\n",
    "\n",
    "\n",
    "# Set up the encoder. Here we are using a single unidirectional model.\n",
    "# To use a bidirectional model as well, call load_model() again with\n",
    "# configuration.model_config(bidirectional_encoder=True) and paths to the\n",
    "# bidirectional model's files. The encoder will use the concatenation of\n",
    "# all loaded models.\n",
    "encoder = encoder_manager.EncoderManager()\n",
    "encoder.load_model(configuration.model_config(),\n",
    "                   vocabulary_file=VOCAB_FILE,\n",
    "                   embedding_matrix_file=EMBEDDING_MATRIX_FILE,\n",
    "                   checkpoint_path=CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/Desktop/NLFF/model/skip_thoughts/skip_thoughts_encoder.py:257: RuntimeWarning: invalid value encountered in true_divide\n",
      "  thought_vectors = [v / np.linalg.norm(v) for v in thought_vectors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "1000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "2000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "3000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "4000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "5000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "6000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "7000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "8000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "9000\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "--------- Fail\n",
      "Failed: 142\n"
     ]
    }
   ],
   "source": [
    "tickers = ['AAPL','AMZN','GOOGL','MSFT','FB','INTC','CSCO','CMCSA','NVDA','NFLX']     \n",
    "tickers = ['AAPL']\n",
    "for ticker in tickers:\n",
    "    news = pd.read_csv('/home/simone/Desktop/NLFF/intrinioDataset/preprocessing/preprocessed/' + ticker + '.csv')\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "\n",
    "    news['EMBEDDING'] = ''\n",
    "    failed = 0\n",
    "\n",
    "    for index, row in news.iterrows():\n",
    "        if(not pd.isnull(news['SUMMARY'][index])):\n",
    "            enc = encoder.encode([news['SUMMARY'][index]])\n",
    "            if(not ( np.isnan(enc[0][0]) or pd.isnull(news['SUMMARY'][index] ))):     \n",
    "                news.at[index, 'EMBEDDING'] = enc[0]\n",
    "            else:\n",
    "                failed += 1\n",
    "                print('--------- Fail')\n",
    "            if(index % 1000 == 0):\n",
    "                print(index)\n",
    "\n",
    "    print('Failed: '+str(failed))           \n",
    "\n",
    "    news = news[['PUBLICATION_DATE', 'SUMMARY', 'EMBEDDING']]\n",
    "    news.to_json(str(ticker)+'_EMBEDDING.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list()\n",
    "for v in news['EMBEDDING'].tolist():\n",
    "    if not sum(v):\n",
    "        a.append(v)\n",
    "    else:\n",
    "        a.append(v[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news = news[np.isfinite(a)]\n",
    "\n",
    "news = news[[not pd.isnull(v) for v in news['EMBEDDING'].tolist()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[cols_to_keep].to_csv('AAPL_EMBEDDING.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# news = pd.read_json('AAPL_EMBEDDING.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news['EMBEDDING'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in news.iterrows():\n",
    "    print(news['EMBEDDING'][i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
