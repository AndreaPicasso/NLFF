{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "skip_vector_dim = 7\n",
    "n_y = 1 #Numero di output, Per ora sali / scendi poi metteremo neutrale\n",
    "\n",
    "\n",
    "class MyModel():\n",
    "    \n",
    "    def __init__(self, num_lstm_units, num_lstm_layers, news_per_hour, learning_rate = 0.009,\n",
    "              num_epochs = 30, minibatch_size = 128):\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.news_per_hour = news_per_hour\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "        \n",
    "\n",
    "    ## Managing state through batches:\n",
    "    def get_state_variables(self, state_placeholder):\n",
    "        l = tf.unstack(state_placeholder, axis=0)\n",
    "        rnn_tuple_state = tuple(\n",
    "        [tf.nn.rnn_cell.LSTMStateTuple(l[idx][0], l[idx][1])\n",
    "         for idx in range(self.num_lstm_layers)]\n",
    "        )\n",
    "        return rnn_tuple_state\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        if(self.num_lstm_layers == 1):\n",
    "            return np.zeros([self.num_lstm_layers, 2, 1, self.num_lstm_units])\n",
    "\n",
    "        return tuple([tf.nn.rnn_cell.LSTMStateTuple(np.zeros([1, 1, self.num_lstm_units]), np.zeros([1, 1, self.num_lstm_units]))for idx in range(self.num_lstm_layers)])\n",
    "\n",
    "\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, self.news_per_hour, skip_vector_dim), name='X')\n",
    "        Y = tf.placeholder(tf.float32, shape=(None, n_y), name='Y')\n",
    "        lstm_state_placeholder = tf.placeholder(tf.float32, [self.num_lstm_layers, 2, None, self.num_lstm_units],  name='lstm_state')\n",
    "        \n",
    "        return X, Y, lstm_state_placeholder\n",
    "\n",
    "\n",
    "\n",
    "    def forward_propagation(self, X, init_state = None):\n",
    "\n",
    "        if init_state != None:\n",
    "            init_state = self.get_state_variables(init_state)\n",
    "\n",
    "        # ATTENTION\n",
    "        e = tf.layers.dense(inputs=X, units=1, activation=tf.nn.relu)\n",
    "        alpha = tf.nn.softmax(e, name='attention_weights')   \t\t\t\t\t\t\t\t\t\t\t\t# tf.nn.softmax(logits,axis=None, ..)\n",
    "        timeSlotEmbeddings =  tf.matmul(alpha, X, transpose_a=True, name='timeSlotEmbeddings')\t\t\t\t# tf.matmul(a,b, transpose_a=False, transpose_b=False, name=None )\n",
    "\n",
    "        # # LSTM\n",
    "        # # (see https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/)\n",
    "\n",
    "        timeSlotSequence = timeSlotEmbeddings\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 1 sequenza di ? sample timeSlotEmbeddings.shape = (?, 1, 2400) -> memoria tra i vari sample\n",
    "\n",
    "\n",
    "        # VEDERE COME FUNZIONA BACKPROP THROUGH TIME PER VEDERE QUALE E MEGLIO\n",
    "        #timeSlotSequence = tf.transpose(timeSlotEmbeddings, perm=[1, 0, 2])\t\t\t\t\t\t\t\t# ? sequenze di 1 sample -> NON c'e memoria esplicita tra i vari sample\n",
    "\n",
    "        #lstm_layer = tf.contrib.rnn.BasicLSTMCell(num_lstm_units)\t\t\t\t\t\t\t\t\t\t\t# Definisco il layer\n",
    "        lstm_layer = tf.contrib.rnn.LSTMCell(self.num_lstm_units, use_peepholes=True)\t\t\t\t\t\t# Definisco il layer\n",
    "\n",
    "        lstm_network = tf.contrib.rnn.MultiRNNCell([lstm_layer] * self.num_lstm_layers)\n",
    "\n",
    "\n",
    "        outputs, new_states = tf.nn.dynamic_rnn(lstm_network, timeSlotSequence,\n",
    "             initial_state=init_state,  dtype=\"float32\",time_major=True)\t\t\t\t\t\t\t\t\t# Definisco la rete ricorrente tramite il layer precedente\n",
    "\n",
    "\n",
    "        outputs = tf.squeeze(outputs, axis=1)\n",
    "    #\t\toutputs = tf.squeeze(outputs, axis=0)\n",
    "\n",
    "        prediction = tf.layers.dense(outputs, 1, activation=tf.nn.sigmoid)\n",
    "\n",
    "\n",
    "        return prediction, new_states\n",
    "\n",
    "\n",
    "\n",
    "    def compute_cost(self, Y_hat, Y):\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_hat, labels = Y))\t\t# Y * -log(sigmoid(Y_hat)) + (1 - Y) * -log(1 - sigmoid(Y_hat))\n",
    "        #cost = tf.reduce_mean(tf.squared_difference(Y_hat, Y))\n",
    "        return cost\n",
    "\n",
    "\n",
    "\n",
    "    def random_mini_batches(self, X_train, Y_train):\n",
    "        minibatches = list()\n",
    "\n",
    "        m = int(len(X_train))\n",
    "        if(self.minibatch_size > m):\n",
    "            minibatches.append((X_train, Y_train))\n",
    "            return minibatches\n",
    "\n",
    "        minibatches.append((X_train[0:self.minibatch_size], Y_train[0:self.minibatch_size]))\n",
    "        iterSize = self.minibatch_size\n",
    "        while(iterSize < m):\n",
    "            if(iterSize+self.minibatch_size < m):\n",
    "                minibatches.append((X_train[iterSize:iterSize+self.minibatch_size], Y_train[iterSize:iterSize+self.minibatch_size]))\n",
    "                iterSize += self.minibatch_size\n",
    "            else:\n",
    "                minibatches.append((X_train[iterSize:m],Y_train[iterSize:m]))\n",
    "                iterSize = m\n",
    "        return minibatches\n",
    "\n",
    "\n",
    "    def run(self, X_train, Y_train, X_dev, Y_dev, X_test=None, Y_test=None, set_verbosity=True):\n",
    "\n",
    "        \n",
    "        if(set_verbosity):\n",
    "            print (\"Learning rate: \" +str(self.learning_rate))\n",
    "\n",
    "        ops.reset_default_graph()                      \t\t\t\t\t\t\t\t\t\t# to be able to rerun the model without overwriting tf variables\n",
    "        m =int(len(X_train))\n",
    "        costs_train = []\n",
    "        costs_dev = []\n",
    "        cost_minibatches=[]\n",
    "        accuracy_train = []\n",
    "        accuracy_dev = []\n",
    "\n",
    "        # Create Placeholders of the correct shape\n",
    "        X, Y, lstm_state_placeholder = self.create_placeholders()\n",
    "\n",
    "        # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "        prediction, lstm_next_state = self.forward_propagation(X, lstm_state_placeholder)\t\n",
    "\n",
    "\n",
    "        # Cost function: Add cost function to tensorflow graph\n",
    "        cost = self.compute_cost(prediction, Y)\n",
    "\n",
    "        # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(cost)\n",
    "        #grads = tf.train.AdamOptimizer(learning_rate = learning_rate).compute_gradients(cost)\n",
    "\n",
    "        # Initialize all the variables globally\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        #This is for computing the test accuracy every epoch\n",
    "        predict_op = tf.to_float(prediction > 0.5)\n",
    "        correct_prediction = tf.equal(predict_op, Y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "        # Start the session to compute the tensorflow graph\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "\n",
    "          # Do the training loop\n",
    "            for epoch in range(self.num_epochs):\n",
    "                # 1 perche per ora ho 1 sola sequenza\n",
    "                lstm_state = self.get_initial_state()\n",
    "                #lstm_state = np.zeros([num_lstm_layers, 2, 1, num_lstm_units])\t\t\t\t\t\t\t\t\t\t\t\t\t\t#Ogni epoch reinizializzo stato\n",
    "\n",
    "                minibatch_cost = 0.0\n",
    "                num_minibatches = int(m / self.minibatch_size)\n",
    "                if(num_minibatches == 0):\n",
    "                    num_minibatches = 1\n",
    "                minibatches = self.random_mini_batches(X_train, Y_train)\n",
    "\n",
    "\n",
    "                for minibatch in minibatches:\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "                    minibatch_X = np.asarray(minibatch_X)\n",
    "                    minibatch_Y = np.asarray(minibatch_Y).reshape((len(minibatch_Y), 1))\n",
    "\n",
    "\n",
    "\n",
    "                    # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                    _ , temp_cost, lstm_state = sess.run([optimizer, cost, lstm_next_state], feed_dict={X: minibatch_X, Y: minibatch_Y, lstm_state_placeholder: lstm_state})\n",
    "\n",
    "\n",
    "                    # weights = tf.get_default_graph().get_tensor_by_name('dense_1/kernel:0')\n",
    "                    #print('state: ' +str(new_state))\n",
    "\n",
    "                    minibatch_cost += temp_cost / num_minibatches\n",
    "\n",
    "                # Print the cost every epoch\n",
    "                if  epoch % 1 == 0:\n",
    "\n",
    "                    lstm_temp_state = self.get_initial_state()\n",
    "                    trainCost, lstm_temp_state = sess.run([cost, lstm_next_state], feed_dict={X: np.asarray(X_train), Y: np.asarray(Y_train).reshape((len(Y_train), 1)), lstm_state_placeholder: lstm_temp_state})\n",
    "                    costs_train.append(trainCost)\n",
    "                    devCost = sess.run(cost, feed_dict={X: np.asarray(X_dev), Y: np.asarray(Y_dev).reshape((len(Y_dev), 1)), lstm_state_placeholder: lstm_temp_state})\n",
    "                    costs_dev.append(devCost)\n",
    "\n",
    "                    lstm_temp_state  = self.get_initial_state()\n",
    "                    trainAccuracy, lstm_temp_state = sess.run([accuracy, lstm_next_state], feed_dict={X: np.asarray(X_train), Y: np.asarray(Y_train).reshape((len(Y_train), 1)),lstm_state_placeholder: lstm_temp_state})\n",
    "                    devAccuracy = sess.run(accuracy, feed_dict={X: np.asarray(X_dev), Y: np.asarray(Y_dev).reshape((len(Y_dev), 1)),lstm_state_placeholder: lstm_temp_state})\n",
    "                    accuracy_train.append(float(trainAccuracy))\n",
    "                    accuracy_dev.append(float(devAccuracy))\n",
    "                if  set_verbosity and epoch % 5 == 0:\n",
    "                    print('miniCost ='+str(minibatch_cost))\n",
    "                    print(\"Epoch \"+str(epoch)+\": \\tTrain cost: \"+str(trainCost)+\" \\tDev cost: \"+str(devCost)+\" \\tTrain Accuracy: \"+str(trainAccuracy)+\" \\tDev accuracy: \"+str(devAccuracy))\n",
    "\n",
    "\n",
    "\n",
    "        if(X_test != None and Y_test != None):\n",
    "            costs_train = []\n",
    "            costs_test = []\n",
    "            cost_minibatches=[]\n",
    "            accuracy_train = []\n",
    "            accuracy_test = []\n",
    "            #Retrain untill the optimal num_epochs accuracy on the dev\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                for epoch in range(np.argmax(accuracy_dev)):\n",
    "                    lstm_state = self.get_initial_state()\n",
    "                    minibatch_cost = 0.0\n",
    "                    num_minibatches = int(m / self.minibatch_size)\n",
    "                    if(num_minibatches == 0):\n",
    "                        num_minibatches = 1\n",
    "                    minibatches = self.random_mini_batches(X_train, Y_train)\n",
    "                    for minibatch in minibatches:\n",
    "                        (minibatch_X, minibatch_Y) = minibatch\n",
    "                        minibatch_X = np.asarray(minibatch_X)\n",
    "                        minibatch_Y = np.asarray(minibatch_Y).reshape((len(minibatch_Y), 1))\n",
    "                        _ , temp_cost, lstm_state = sess.run([optimizer, cost, lstm_next_state], feed_dict={X: minibatch_X, Y: minibatch_Y, lstm_state_placeholder: lstm_state})\n",
    "                        minibatch_cost += temp_cost / num_minibatches\n",
    "                    if  epoch % 1 == 0:\n",
    "                        lstm_temp_state = self.get_initial_state()\n",
    "                        trainCost, lstm_temp_state = sess.run([cost, lstm_next_state], feed_dict={X: np.asarray(X_train), Y: np.asarray(Y_train).reshape((len(Y_train), 1)), lstm_state_placeholder: lstm_temp_state})\n",
    "                        costs_train.append(trainCost)\n",
    "                        devCost, lstm_temp_state = sess.run([cost, lstm_next_state], feed_dict={X: np.asarray(X_dev), Y: np.asarray(Y_dev).reshape((len(Y_dev), 1)), lstm_state_placeholder: lstm_temp_state})\n",
    "                        testCost = sess.run(cost, feed_dict={X: np.asarray(X_test), Y: np.asarray(Y_test).reshape((len(Y_test), 1)), lstm_state_placeholder: lstm_temp_state})\n",
    "                        costs_test.append(testCost)\n",
    "\n",
    "                        lstm_temp_state  = self.get_initial_state()\n",
    "                        trainAccuracy, lstm_temp_state = sess.run([accuracy, lstm_next_state], feed_dict={X: np.asarray(X_train), Y: np.asarray(Y_train).reshape((len(Y_train), 1)),lstm_state_placeholder: lstm_temp_state})\n",
    "                        devAccuracy, lstm_temp_state = sess.run([accuracy, lstm_next_state], feed_dict={X: np.asarray(X_dev), Y: np.asarray(Y_dev).reshape((len(Y_dev), 1)),lstm_state_placeholder: lstm_temp_state})\n",
    "                        testAccuracy = sess.run(accuracy, feed_dict={X: np.asarray(X_test), Y: np.asarray(Y_test).reshape((len(Y_test), 1)),lstm_state_placeholder: lstm_temp_state})\n",
    "                        accuracy_train.append(float(trainAccuracy))\n",
    "                        accuracy_test.append(float(testAccuracy))\n",
    "                    if  set_verbosity and epoch % 5 == 0:\n",
    "                        print('miniCost ='+str(minibatch_cost))\n",
    "                        print(\"Epoch \"+str(epoch)+\": \\tTrain cost: \"+str(trainCost)+\" \\tTest cost: \"+str(testCost)+\" \\tTrain Accuracy: \"+str(trainAccuracy)+\" \\tTest accuracy: \"+str(testAccuracy))\n",
    "\n",
    "                if(set_verbosity):\n",
    "                    plt.figure(figsize=(20,10))\n",
    "                    plt.plot(range(0,len(accuracy_train)), accuracy_train,'b', label='accuracy_train')\n",
    "                    plt.plot(range(0,len(accuracy_test)), accuracy_test,'r', label='accuracy_test')\n",
    "                    plt.plot(range(0,len(costs_train)),costs_train,'--b', label='cost_train')\n",
    "                    plt.plot(range(0,len(costs_test)),costs_test,'--r', label='cost_test' )\n",
    "\n",
    "                    plt.ylabel('accuracy')\n",
    "                    plt.xlabel('epochs')\n",
    "                    plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    print(\"Train Accuracy:\", accuracy_train[-1])\n",
    "                    print(\"Test Accuracy:\",  accuracy_test[-1])\n",
    "                return (accuracy_train[-1], accuracy_test[-1])\n",
    "            \n",
    "        else:\n",
    "            if(set_verbosity):\n",
    "                print(\"Train Accuracy:\", max(accuracy_train))\n",
    "                print(\"Dev Accuracy:\",  max(accuracy_dev))\n",
    "            return (max(accuracy_train), max(accuracy_dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection():\n",
    "\n",
    "    def modelSelection(iterations = 400, learning_rate = 0.001, minibatch_size = 512 ):\n",
    "        import os\n",
    "        # fold = list(train_i , dev_i)\n",
    "        # train_i = (train_x, train_y) \n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "        best_NumLSTMUnits = 0\n",
    "        best_news_per_hour = 0\n",
    "        best_X_window_average = 0\n",
    "        best_newsTimeToMarket = 0\n",
    "        \n",
    "        \n",
    "        bestAccuracy = 0\n",
    "        # Each day: 7 hours of trading\n",
    "        for i in range(0,iterations):\n",
    "            news_per_hour = np.asscalar(np.random.random_integers(10, 40))\n",
    "            X_window_average = np.asscalar(np.random.random_integers(10, 40))\n",
    "            newsTimeToMarket = np.asscalar(np.random.random_integers(0, 50))\n",
    "            # newsTimeToMarket = 0\n",
    "            numLSTMUnits = np.asscalar(np.random.random_integers(100, 1000))\n",
    "\n",
    "            print('NewsHour: '+str(news_per_hour)+' \\tXavg: '+str(X_window_average)+' \\t newsTTM: '+str(newsTimeToMarket)+\n",
    "                  '\\t LSTMUnits: '+str(numLSTMUnits)+'\\t lr: '+str(learning_rate)+'\\t miniBatch: '+str(minibatch_size))\n",
    "            \n",
    "            \n",
    "            Data.load_data(news_per_hour = news_per_hour, X_window_average=X_window_average,\n",
    "                           momentum_window=30, newsTimeToMarket = newsTimeToMarket, set_verbosity=False)\n",
    "\n",
    "            folds = Data.get_cross_validation_train_dev_set(test_percentage=0.3, k_fold = 5,  dev_num_points=100)\n",
    "\n",
    "            k_fold_train_accuracy = []\n",
    "            k_fold_dev_accuracy = []\n",
    "            always_yes = []\n",
    "\n",
    "            for fold in folds:\n",
    "                print(\"==\", end=\"\", flush=True)\n",
    "                (X_train, Y_train), (X_dev, Y_dev) = fold\n",
    "                model = MyModel(num_lstm_units=numLSTMUnits, num_lstm_layers=1, news_per_hour = news_per_hour,\n",
    "                                learning_rate = learning_rate, minibatch_size = minibatch_size,\n",
    "                                num_epochs = 30)\n",
    "                \n",
    "                (train_accuracy, dev_accuracy) = model.run(X_train=X_train, Y_train=Y_train,X_dev=X_dev,\n",
    "                                                           Y_dev=Y_dev, set_verbosity=False)\n",
    "\n",
    "                k_fold_train_accuracy.append(train_accuracy)\n",
    "                k_fold_dev_accuracy.append(dev_accuracy)\n",
    "                always_yes.append(np.sum(np.asarray(Y_dev)==1)/len(Y_dev))\n",
    "            print('( train_fold_accuracy: '+str(sum(k_fold_train_accuracy) / len(folds))+ ', dev_fold_accuracy: '+str(sum(k_fold_dev_accuracy) / len(folds))+' )')\n",
    "            print('Dev accuracies: '+str(k_fold_dev_accuracy))\n",
    "            print('Dev predict y=1: '+str(always_yes))\n",
    "            print('( train_fold_variance: '+str(np.var(np.asarray(k_fold_train_accuracy)))+ ', dev_fold_variance: '+str(np.var(np.asarray(k_fold_dev_accuracy)))+' )')\n",
    "\n",
    "            if(dev_accuracy > bestAccuracy):\n",
    "                best_NumLSTMUnits = numLSTMUnits\n",
    "                best_news_per_hour = news_per_hour\n",
    "                best_X_window_average = X_window_average\n",
    "                best_newsTimeToMarket = newsTimeToMarket\n",
    "\n",
    "        print('#################################\\n')\n",
    "        print('BEST:')\n",
    "        print('best_NumLSTMUnits: '+str(best_NumLSTMUnits))\n",
    "        print('best_news_per_hour: '+str(best_news_per_hour))\n",
    "        print('best_X_window_average: '+str(best_X_window_average))\n",
    "        print('best_newsTimeToMarket: '+str(best_newsTimeToMarket))\n",
    "        \n",
    "        print('\\n#################################\\n')\n",
    "                    \n",
    "        print('Training optimal model...')\n",
    "        Data.load_data(news_per_hour = best_news_per_hour, X_window_average=best_X_window_average,\n",
    "                       momentum_window=30, newsTimeToMarket = best_newsTimeToMarket)\n",
    "        _ , (X_test, Y_test) = Data.get_train_test_set()\n",
    "        \n",
    "        #For training I take the biggest fold, validation on dev for early stopping (opt num epochs)\n",
    "        folds = Data.get_cross_validation_train_dev_set(test_percentage=0.3, k_fold = 5,  dev_num_points=100)\n",
    "        (X_train, Y_train), (X_dev, Y_dev) = folds[-1] \n",
    "        \n",
    "        model = MyModel(num_lstm_units=best_NumLSTMUnits, num_lstm_layers=1, news_per_hour = best_news_per_hour,\n",
    "                        learning_rate = learning_rate, minibatch_size = minibatch_size, num_epochs = 30)\n",
    "        \n",
    "        model.run(X_train, Y_train,X_dev, Y_dev, X_test, Y_test)\n",
    "        \n",
    "\n",
    "    def modelSelectionFixedTTM(ticker='AAPL', iterations = 100, learning_rate = 0.001, minibatch_size = 512):\n",
    "        import os\n",
    "        # fold = list(train_i , dev_i)\n",
    "        # train_i = (train_x, train_y) \n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "        best_NumLSTMUnits = []\n",
    "        best_news_per_hour = []\n",
    "        best_X_window_average = []\n",
    "        \n",
    "        Ttm_range = [0, 7, 14, 21, 35, 70, 105,210]\n",
    "\n",
    "        \n",
    "        bestAccuracy = 0\n",
    "        # Each day: 7 hours of trading\n",
    "        for newsTimeToMarketIndex in range(0, len(Ttm_range)):\n",
    "            newsTimeToMarket = Ttm_range[newsTimeToMarketIndex]\n",
    "            \n",
    "            best_NumLSTMUnits.append(0)\n",
    "            best_news_per_hour.append(0)\n",
    "            best_X_window_average.append(0)\n",
    "            \n",
    "#             print('\\n\\n\\n *****************  Predicting the future at '+str(newsTimeToMarket/7)+' days ****************')\n",
    "            for i in range(0,iterations):\n",
    "                news_per_hour = np.asscalar(np.random.random_integers(10, 40))\n",
    "                X_window_average = np.asscalar(np.random.random_integers(10, 100))\n",
    "                numLSTMUnits = np.asscalar(np.random.random_integers(50, 1000))\n",
    "\n",
    "\n",
    "                Data.load_data(ticker= ticker, news_per_hour = news_per_hour, X_window_average=X_window_average,\n",
    "                               momentum_window=30, newsTimeToMarket = newsTimeToMarket, set_verbosity=False)\n",
    "\n",
    "                folds = Data.get_cross_validation_train_dev_set(test_percentage=0.3, k_fold = 5,  dev_num_points=100)\n",
    "\n",
    "#                 print('NewsHour: '+str(news_per_hour)+' \\tXavg: '+str(X_window_average)+' \\t newsTTM: '+str(newsTimeToMarket)+\n",
    "#                   '\\t LSTMUnits: '+str(numLSTMUnits)+'\\t lr: '+str(learning_rate)+'\\t miniBatch: '+str(minibatch_size))\n",
    "            \n",
    "                k_fold_train_accuracy = []\n",
    "                k_fold_dev_accuracy = []\n",
    "                always_yes = []\n",
    "\n",
    "                for fold in folds:\n",
    "                    print(\"==\", end=\"\", flush=True)\n",
    "                    (X_train, Y_train), (X_dev, Y_dev) = fold\n",
    "                    model = MyModel(num_lstm_units=numLSTMUnits, num_lstm_layers=1, news_per_hour = news_per_hour,\n",
    "                                learning_rate = learning_rate, num_epochs = 30, minibatch_size = minibatch_size)\n",
    "                \n",
    "                    (train_accuracy, dev_accuracy) = model.run(X_train,Y_train,X_dev,Y_dev, set_verbosity=False)\n",
    "                    \n",
    "                    \n",
    "                    k_fold_train_accuracy.append(train_accuracy)\n",
    "                    k_fold_dev_accuracy.append(dev_accuracy)\n",
    "                    always_yes.append(np.sum(np.asarray(Y_dev)==1)/len(Y_dev))\n",
    "#                 print('( train_fold_accuracy: '+str(sum(k_fold_train_accuracy) / len(folds))+ ', dev_fold_accuracy: '+str(sum(k_fold_dev_accuracy) / len(folds))+' )')\n",
    "#                 print('( train_fold_variance: '+str(np.var(np.asarray(k_fold_train_accuracy)))+ ', dev_fold_variance: '+str(np.var(np.asarray(k_fold_dev_accuracy)))+' )')\n",
    "\n",
    "            if(dev_accuracy > bestAccuracy):\n",
    "                best_NumLSTMUnits[newsTimeToMarketIndex] = numLSTMUnits\n",
    "                best_news_per_hour[newsTimeToMarketIndex] = news_per_hour\n",
    "                best_X_window_average[newsTimeToMarketIndex] = X_window_average\n",
    "                \n",
    "\n",
    "        print('\\n#################################\\n')\n",
    "        print('BEST:')\n",
    "        print('Predicting the future at: '+str(np.asarray(Ttm_range)/7)+' days')\n",
    "        print('best_NumLSTMUnits: '+str(best_NumLSTMUnits))\n",
    "        print('best_news_per_hour: '+str(best_news_per_hour))\n",
    "        print('best_X_window_average: '+str(best_X_window_average))\n",
    "        print('\\n#################################\\n')\n",
    "                    \n",
    "        print('Training optimal model...')\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for newsTimeToMarketIndex in range(0, len(Ttm_range)):\n",
    "            newsTimeToMarket = Ttm_range[newsTimeToMarketIndex]\n",
    "            Data.load_data(news_per_hour = best_news_per_hour[newsTimeToMarketIndex],\n",
    "                           X_window_average=best_X_window_average[newsTimeToMarketIndex],\n",
    "                           momentum_window=30, newsTimeToMarket = newsTimeToMarket, set_verbosity=False)\n",
    "            _ , (X_test, Y_test) = Data.get_train_test_set()\n",
    "            \n",
    "            #For training I take the biggest fold, validation on dev for early stopping (opt num epochs)\n",
    "            folds = Data.get_cross_validation_train_dev_set(test_percentage=0.3, k_fold = 5,  dev_num_points=100)\n",
    "            (X_train, Y_train), (X_dev, Y_dev) = folds[-1] \n",
    "            \n",
    "            model = MyModel(num_lstm_units=best_NumLSTMUnits[newsTimeToMarketIndex], num_lstm_layers=1,\n",
    "                            news_per_hour = best_news_per_hour[newsTimeToMarketIndex],learning_rate = learning_rate,\n",
    "                            num_epochs = 30, minibatch_size = minibatch_size)\n",
    "                \n",
    "            (train_accuracy, test_accuracy) = model.run(X_train,Y_train, X_dev, Y_dev, X_test, Y_test, set_verbosity=False)\n",
    "            \n",
    "            train_accs.append(train_accuracy)\n",
    "            test_accs.append(test_accuracy)\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(Ttm_range,train_accs,'b', label='accuracy_train')\n",
    "        plt.plot(Ttm_range,test_accs,'r', label='accuracy_test' )\n",
    "\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('hours in the future')\n",
    "        #plt.title(\"Accuracy moving the window\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREV---------------------------------\n",
    "# X_path = '/home/simone/Desktop/wordCountEmbedding.json'\n",
    "# Y_path = '../../DataSetIndexes/indexesAAPL.csv'\n",
    "# - X -> read csv\n",
    "# - model sel TTM -> test verbosity false\n",
    "# - all params for the ticker\n",
    "# - removed +%f from x to datetime\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    elif x < 0:\n",
    "        #return -1\n",
    "        return 0\n",
    "    \n",
    "class Data():\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "\n",
    "    def get_train_test_set(test_percentage=0.3):\n",
    "        idx_split = math.floor(len(Data.X)*(1-test_percentage))\n",
    "\n",
    "        train_x = Data.X[:idx_split]\n",
    "        train_y = Data.Y[:idx_split]\n",
    "        test_x = Data.X[idx_split:]\n",
    "        test_y = Data.Y[idx_split:]\n",
    "\n",
    "        return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "\n",
    "    def get_cross_validation_train_dev_set(test_percentage=0.3, k_fold = 10,  dev_num_points=100):\n",
    "\n",
    "        # https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection/14109#14109\n",
    "\n",
    "\n",
    "        # fold = list(train_i , dev_i)\n",
    "        # train_i = (train_x, train_y) \n",
    "        (train_x, train_y), _  = Data.get_train_test_set(test_percentage=test_percentage)\n",
    "        m = int(len(train_x))\n",
    "        samples_per_fold = int( (m - dev_num_points) / k_fold)\n",
    "        # print('Fold lenght: '+str(samples_per_fold))\n",
    "        fold = list()\n",
    "        index = 0\n",
    "        while(len(fold) < k_fold):\n",
    "            # Training folds of equal length:\n",
    "#           fold.append(((train_x[index:index+samples_per_fold], train_y[index:index+samples_per_fold]),\n",
    "#                 (train_x[index+samples_per_fold:index+samples_per_fold+dev_num_points], train_y[index+samples_per_fold:index+samples_per_fold+dev_num_points])))\n",
    "            fold.append(((train_x[0:index+samples_per_fold], train_y[0:index+samples_per_fold]),\n",
    "                (train_x[index+samples_per_fold:index+samples_per_fold+dev_num_points], train_y[index+samples_per_fold:index+samples_per_fold+dev_num_points])))\n",
    "            index += samples_per_fold\n",
    "\n",
    "        return fold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(ticker='AAPL', momentum_window=30, X_window_average=None, news_per_hour = 10, newsTimeToMarket = 0, set_verbosity=True):\n",
    "        X_path = '/home/simone/Desktop/NLFF/sentiment/myTools/wordCount+KSVM/SentimentSingleNews/'+str(ticker)+'.csv'\n",
    "        Y_path = '../../DataSetIndexes/indexes'+str(ticker)+'.csv'\n",
    "        \n",
    "        \n",
    "        if(set_verbosity):\n",
    "            print('Reading dataset...')\n",
    "            \n",
    "        x = pd.read_csv(X_path)\n",
    "        x = x.rename(index=str, columns={\"initTime\": \"PUBLICATION_DATE\"})\n",
    "        #cambio l'ordine dalla piu vecchia alla piu recente\n",
    "        if(set_verbosity):\n",
    "            print('Ordering dataset...')\n",
    "        x = x.sort_values(by=['PUBLICATION_DATE'])\n",
    "        x = x.reset_index(drop=True)\n",
    "        \n",
    "        if(X_window_average != None):\n",
    "            if(set_verbosity):\n",
    "                print('Moving average..')\n",
    "            x['CONSTRAINING'] = x['CONSTRAINING'].rolling(window=X_window_average,center=False).mean()\n",
    "            x['LITIGIOUS'] = x['LITIGIOUS'].rolling(window=X_window_average,center=False).mean()\n",
    "            x['NEGATIVE'] = x['NEGATIVE'].rolling(window=X_window_average,center=False).mean()\n",
    "            x['POSITIVE'] = x['POSITIVE'].rolling(window=X_window_average,center=False).mean()\n",
    "            x['UNCERTAINTY'] = x['UNCERTAINTY'].rolling(window=X_window_average,center=False).mean()\n",
    "            x['SUPERFLUOUS'] = x['SUPERFLUOUS'].rolling(window=X_window_average,center=False).mean()\n",
    "            x['INTERESTING'] = x['INTERESTING'].rolling(window=X_window_average,center=False).mean()\n",
    "            \n",
    "            x.drop(np.arange(X_window_average-1), inplace=True)\n",
    "            x = x.reset_index(drop=True)\n",
    "        \n",
    "        #Normalizzo\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x[['CONSTRAINING', 'LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING']] = min_max_scaler.fit_transform(x[['CONSTRAINING', 'LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING']].values)\n",
    "            \n",
    "\n",
    "        for i, row in x.iterrows():\n",
    "            x.at[i,'PUBLICATION_DATE'] =datetime.strptime(x['PUBLICATION_DATE'][i], '%Y-%m-%d %H:%M:%S') + timedelta(minutes=newsTimeToMarket)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        y = pd.read_csv(Y_path)\n",
    "        y = y.rename(index=str, columns={\"Unnamed: 0\": \"DATE\"})\n",
    "\n",
    "        #PER ORA SCARTO GLI INDICI, POI SARA' DA METTERLI DENTRO X\n",
    "        #y = y['DATE', 'close']\n",
    "        for i, row in y.iterrows():\n",
    "            y['DATE'].at[i] = datetime.strptime(y['DATE'][i], '%Y-%m-%d %H:%M:%S') \n",
    "\n",
    "        z = list()\n",
    "        if(set_verbosity):\n",
    "            print('y(t) - y(t-1) ...')\n",
    "\n",
    "        #calcolo differenza price(t) - price(t-window)\n",
    "        for i in range(0,momentum_window):\n",
    "            z.append(523) #Valore impossibile per fare drop successivamente  \n",
    "        for i in range(momentum_window,y.shape[0]):\n",
    "            z.append(sign(y['close'][i] - y['close'][i-momentum_window]))\n",
    "        y['close'] = z\n",
    "\n",
    "        y = y[y['close'] != 523] #Ellimino primi valori per momentum window\n",
    "\n",
    "        if(set_verbosity):\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(np.arange(0, len(x)), x['CONSTRAINING'],'b')\n",
    "            plt.plot(np.arange(0, len(x)), x['LITIGIOUS'],'g')\n",
    "            plt.plot(np.arange(0, len(x)), x['NEGATIVE'],'m')\n",
    "            plt.plot(np.arange(0, len(x)), x['POSITIVE'],'c')\n",
    "            plt.plot(np.arange(0, len(x)), x['UNCERTAINTY'],'r')\n",
    "            plt.plot(np.arange(0, len(x)), x['SUPERFLUOUS'],'y')\n",
    "            plt.plot(np.arange(0, len(x)), x['INTERESTING'],'k')\n",
    "            plt.ylabel('INPUT')\n",
    "            plt.xlabel('time')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        \n",
    "        if(set_verbosity):\n",
    "            print('Alligning dataset and constructing cube..')\n",
    "\n",
    "        initDate = max(y['DATE'][0], x['PUBLICATION_DATE'][0])\n",
    "        finalDate = min(y['DATE'][len(y)-1], x['PUBLICATION_DATE'][len(x)-1])\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # ALLINEAMENTO INIZIO\n",
    "        while(y['DATE'][j] < initDate):\n",
    "            j+=1\n",
    "        while(x['PUBLICATION_DATE'][i] < initDate):\n",
    "            i+=1\n",
    "\n",
    "        while(x['PUBLICATION_DATE'][i] < finalDate and y['DATE'][j] < finalDate ):\n",
    "            timeSlotX = list()\n",
    "            while(i<len(x)-1 and y['DATE'][j] > x['PUBLICATION_DATE'][i]):\n",
    "                timeSlotX.append([x['CONSTRAINING'][i],x['LITIGIOUS'][i],x['NEGATIVE'][i],x['POSITIVE'][i],x['UNCERTAINTY'][i],x['SUPERFLUOUS'][i],x['INTERESTING'][i]]) \n",
    "                i+=1\n",
    "\n",
    "\n",
    "            # Da len(timeslot) dobbiamo ricondurci ad avere news_per_hour numero di news\n",
    "            # Random sampling se sono troppe:\n",
    "            if(len(timeSlotX) > news_per_hour):\n",
    "                #timeSlotX = np.random.choice(timeSlotX, news_per_hour, replace=False)\n",
    "                selectedIndexes = np.random.choice(range(0,len(timeSlotX)-1), news_per_hour, replace=False).tolist()\n",
    "                timeSlotX =  [timeSlotX[index] for index in selectedIndexes]\n",
    "                \n",
    "            # Replicazione news se sono troppo poche\n",
    "            else:\n",
    "                if(len(timeSlotX) < news_per_hour):\n",
    "                    index = 0\n",
    "                    #Se non e presente manco una news riempi di zeri\n",
    "                    if(len(timeSlotX) == 0):\n",
    "                        timeSlotX.append([0] * 5)\n",
    "                        \n",
    "                    numNews = len(timeSlotX)\n",
    "\n",
    "                    while(len(timeSlotX) < news_per_hour):\n",
    "                        timeSlotX.append(timeSlotX[index%numNews])\n",
    "    \n",
    "            X.append(timeSlotX)   \n",
    "            Y.append(y['close'][j])\n",
    "            j+=1\n",
    "\n",
    "        Data.X = X\n",
    "        Data.Y = Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ModelSelection.modelSelection(iterations = 1)\n",
    "\n",
    "\n",
    "# #################################\n",
    "\n",
    "# BEST: (learning rate = 0.001, batch = 128 CREDO)\n",
    "# best_NumLSTMUnits: 652\n",
    "# best_news_per_hour: 33\n",
    "# best_X_window_average: 30\n",
    "# best_newsTimeToMarket: 0\n",
    "\n",
    "# Train Accuracy: 0.8452174067497253\n",
    "# Test Accuracy: 0.7707910537719727\n",
    "\n",
    "# #################################\n",
    "\n",
    "\n",
    "tickers = ['AAPL', 'ADBE','GOOGL','NFLX','TSLA', 'MSFT','AMZN','FB', 'ADI', 'ADP', 'ADSK', 'AKAM', 'ALGN','ALXN', 'AMAT',\n",
    "           'AMGN',  'ATVI', 'AVGO', 'BIDU','BIIB', 'BMRN', 'CA', 'CELG', 'CERN', 'CHTR', 'CMCSA',\n",
    "          'COST', 'CSCO', 'CSX', 'CTAS', 'CTRP', 'CTSH', 'CTXS',\n",
    "          'DISCA', 'DISH', 'DLTR', 'EA', 'EBAY', 'ESRX', 'EXPE',\n",
    "          'FAST', 'FISV', 'FOXA', 'GILD',  'HAS',\n",
    "          'HOLX', 'HSIC', 'IDXX', 'ILMN', 'INCY', 'INTC', 'INTU',\n",
    "          'ISRG', 'JBHT', 'JD', 'KHC', 'KLAC', 'LBTYA', 'LRCX', 'MAR',\n",
    "          'MAT', 'MCHP', 'MDLZ', 'MELI', 'MNST',  'MU', 'MXIM',\n",
    "          'MYL',  'NTES', 'NVDA', 'ORLY', 'PAYX', 'PCAR', 'PCLN',\n",
    "          'PYPL', 'QCOM', 'REGN', 'ROST', 'SBUX', 'SHPG', 'SIRI', 'STX',\n",
    "          'SWKS', 'SYMC', 'TMUS', 'TSCO',  'TXN', 'ULTA', 'VIAB',\n",
    "          'VOD', 'VRSK', 'VRTX', 'WBA', 'WYNN', 'XLNX', 'XRAY']\n",
    "\n",
    "for tic in tickers:\n",
    "    print('\\n\\n\\n==================== '+str(tic)+' ==================== \\n\\n\\n')\n",
    "    ModelSelection.modelSelectionFixedTTM(ticker=tic, iterations = 100, learning_rate = 0.001, minibatch_size = 512)\n",
    "\n",
    "# #################################\n",
    "\n",
    "# BEST:\n",
    "# Predicting the future at: [ 0.  1.  2.  3.  5. 10. 15.] days\n",
    "# best_NumLSTMUnits: [360, 292, 720, 237, 724, 204, 817]\n",
    "# best_news_per_hour: [10, 11, 10, 39, 18, 30, 25]\n",
    "# best_X_window_average: [67, 90, 47, 34, 11, 52, 76]\n",
    "\n",
    "# #################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### single run:\n",
    "learning_rate=0.001\n",
    "batch_size=512\n",
    "\n",
    "\n",
    "num_lstm_layers = 1\n",
    "num_lstm_units = 100\n",
    "news_per_hour = 30\n",
    "X_window_average = 10\n",
    "newsTimeToMarket = 0\n",
    "\n",
    "\n",
    "\n",
    "Data.load_data(news_per_hour = news_per_hour,\n",
    "               X_window_average=X_window_average,\n",
    "               momentum_window=30,\n",
    "               newsTimeToMarket = newsTimeToMarket)\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = Data.get_train_test_set()\n",
    "\n",
    "test_x = tf.convert_to_tensor(np.asarray(X_test), dtype=tf.float32)\n",
    "train_x = tf.convert_to_tensor(np.asarray(X_train), dtype=tf.float32)\n",
    "\n",
    "train_y = tf.convert_to_tensor(np.asarray(Y_train), dtype=tf.float32)\n",
    "test_y = tf.convert_to_tensor(np.asarray(Y_test), dtype=tf.float32)\n",
    "\n",
    "print('.........................')\n",
    "print (\"number of training examples = \" + str(train_x.shape[0]))\n",
    "print (\"number of test examples = \" + str(test_x.shape[0]))\n",
    "print (\"X_train shape: \" + str(train_x.shape))\n",
    "print (\"Y_train shape: \" + str(train_y.shape))\n",
    "print (\"X_test shape: \" + str(test_x.shape))\n",
    "print (\"Y_test shape: \" + str(test_y.shape))\n",
    "print (\"Test baseline: \" + str( np.sum(np.asarray(Y_test)==1)/len(Y_test)))\n",
    "print('.........................')\n",
    "\n",
    "\n",
    "model = MyModel(num_lstm_units=num_lstm_units, num_lstm_layers=num_lstm_layers, news_per_hour = news_per_hour,\n",
    "                learning_rate = learning_rate, num_epochs = 30, minibatch_size = batch_size)\n",
    "\n",
    "model.run(X_train, Y_train, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttm_range = [ 0,  1,  2,  3,  5, 10, 15, 30]\n",
    "train_accs = [0.782608687877655, 0.8426086902618408, 0.7408695816993713, 0.8895652294158936, 0.7260869741439819, 0.8408695459365845,0.678260862827301,0.678260862827301]\n",
    "test_accs = [0.8133874535560608, 0.8215010166168213, 0.7606490850448608, 0.8580121994018555, 0.7444218993186951, 0.7910750508308411, 0.5841785073280334, 0.5841785073280334]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(Ttm_range,train_accs,'b', label='accuracy_train')\n",
    "plt.plot(Ttm_range,test_accs,'r', label='accuracy_test' )\n",
    "\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('days in the future')\n",
    "#plt.title(\"Accuracy moving the window\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =pd.read_csv('/home/simone/Desktop/NLFF/indexes/indexesHOLX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "batch_size=512\n",
    "\n",
    "\n",
    "num_lstm_layers = 1\n",
    "num_lstm_units = 100\n",
    "news_per_hour = 30\n",
    "X_window_average = 10\n",
    "newsTimeToMarket = 0\n",
    "\n",
    "\n",
    "\n",
    "Data.load_data(news_per_hour = news_per_hour,\n",
    "               X_window_average=X_window_average,\n",
    "               momentum_window=30,\n",
    "               newsTimeToMarket = newsTimeToMarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
