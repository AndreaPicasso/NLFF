{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordCountSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEGATIVE</th>\n",
       "      <th>POSITIVE</th>\n",
       "      <th>UNCERTAINTY</th>\n",
       "      <th>LITIGIOUS</th>\n",
       "      <th>CONSTRAINING</th>\n",
       "      <th>SUPERFLUOUS</th>\n",
       "      <th>INTERESTING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>ABEYANCE</td>\n",
       "      <td>ABOVEMENTIONED</td>\n",
       "      <td>ABIDE</td>\n",
       "      <td>AEGIS</td>\n",
       "      <td>AGGRESSIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDONED</td>\n",
       "      <td>ABUNDANCE</td>\n",
       "      <td>ABEYANCES</td>\n",
       "      <td>ABROGATE</td>\n",
       "      <td>ABIDING</td>\n",
       "      <td>AMORPHOUS</td>\n",
       "      <td>AGGRESSIVELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONING</td>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>ALMOST</td>\n",
       "      <td>ABROGATED</td>\n",
       "      <td>BOUND</td>\n",
       "      <td>ANTICIPATORY</td>\n",
       "      <td>AGGRESSIVENESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>ACCLAIMED</td>\n",
       "      <td>ALTERATION</td>\n",
       "      <td>ABROGATES</td>\n",
       "      <td>BOUNDED</td>\n",
       "      <td>APPERTAINING</td>\n",
       "      <td>ASBESTOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "      <td>ACCOMPLISH</td>\n",
       "      <td>ALTERATIONS</td>\n",
       "      <td>ABROGATING</td>\n",
       "      <td>COMMIT</td>\n",
       "      <td>ASSIMILATE</td>\n",
       "      <td>AUGUST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NEGATIVE    POSITIVE  UNCERTAINTY       LITIGIOUS CONSTRAINING  \\\n",
       "0       ABANDON        ABLE     ABEYANCE  ABOVEMENTIONED        ABIDE   \n",
       "1     ABANDONED   ABUNDANCE    ABEYANCES        ABROGATE      ABIDING   \n",
       "2    ABANDONING    ABUNDANT       ALMOST       ABROGATED        BOUND   \n",
       "3   ABANDONMENT   ACCLAIMED   ALTERATION       ABROGATES      BOUNDED   \n",
       "4  ABANDONMENTS  ACCOMPLISH  ALTERATIONS      ABROGATING       COMMIT   \n",
       "\n",
       "    SUPERFLUOUS     INTERESTING  \n",
       "0         AEGIS      AGGRESSIVE  \n",
       "1     AMORPHOUS    AGGRESSIVELY  \n",
       "2  ANTICIPATORY  AGGRESSIVENESS  \n",
       "3  APPERTAINING        ASBESTOS  \n",
       "4    ASSIMILATE          AUGUST  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = pd.read_csv('LoughranMcDonald_MyDictionary2.csv')\n",
    "dictionary.drop('Unnamed: 0', 1, inplace=True)\n",
    "\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEGATIVE</th>\n",
       "      <th>POSITIVE</th>\n",
       "      <th>UNCERTAINTY</th>\n",
       "      <th>LITIGIOUS</th>\n",
       "      <th>CONSTRAINING</th>\n",
       "      <th>SUPERFLUOUS</th>\n",
       "      <th>INTERESTING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abl</td>\n",
       "      <td>abey</td>\n",
       "      <td>abovement</td>\n",
       "      <td>abid</td>\n",
       "      <td>aegi</td>\n",
       "      <td>aggress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abund</td>\n",
       "      <td>abey</td>\n",
       "      <td>abrog</td>\n",
       "      <td>abid</td>\n",
       "      <td>amorph</td>\n",
       "      <td>aggress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abund</td>\n",
       "      <td>almost</td>\n",
       "      <td>abrog</td>\n",
       "      <td>bound</td>\n",
       "      <td>anticipatori</td>\n",
       "      <td>aggress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>acclaim</td>\n",
       "      <td>alter</td>\n",
       "      <td>abrog</td>\n",
       "      <td>bound</td>\n",
       "      <td>appertain</td>\n",
       "      <td>asbesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandon</td>\n",
       "      <td>accomplish</td>\n",
       "      <td>alter</td>\n",
       "      <td>abrog</td>\n",
       "      <td>commit</td>\n",
       "      <td>assimil</td>\n",
       "      <td>august</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  NEGATIVE    POSITIVE UNCERTAINTY  LITIGIOUS CONSTRAINING   SUPERFLUOUS  \\\n",
       "0  abandon         abl        abey  abovement         abid          aegi   \n",
       "1  abandon       abund        abey      abrog         abid        amorph   \n",
       "2  abandon       abund      almost      abrog        bound  anticipatori   \n",
       "3  abandon     acclaim       alter      abrog        bound     appertain   \n",
       "4  abandon  accomplish       alter      abrog       commit       assimil   \n",
       "\n",
       "  INTERESTING  \n",
       "0     aggress  \n",
       "1     aggress  \n",
       "2     aggress  \n",
       "3     asbesto  \n",
       "4      august  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountSentiment.lemmatizeDictionary(dictionary)\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONSTRAINING': 0.25,\n",
       " 'LITIGIOUS': 0.25,\n",
       " 'NEGATIVE': 0.0,\n",
       " 'POSITIVE': 0.25,\n",
       " 'UNCERTAINTY': 0.25}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountSentiment.getSentiment('Apple is designing its own power-management chips for use in iPhones as early as 2018, the Nikkei business daily reported on Thursday, triggering a more than 20 percent slide in shares of supplier Dialog Semiconductor.  If confirmed, the move would reduce Apple is dependence on the Anglo-German chipmaker, which itself is heavily reliant on the smartphone industry and has been trying to diversify its customer base.  Investors are particularly jittery after Apple said in April that it planned to replace graphics chip supplier Imagination Technologies, sending the London-listed stock down 70 percent in a single session.',dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tickers=['AAPL','AMZN','PEP','GOOGL','MSFT','FB','INTC','CSCO','CMCSA','NVDA','NFLX','BKNG','ADBE','AMGN','TXN','AVGO','PYPL','GILD','COST','QCOM']       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "ADBE\n",
      "ADI\n",
      "ADP\n",
      "ADSK\n",
      "AKAM\n",
      "ALGN\n",
      "ALXN\n",
      "AMAT\n",
      "AMGN\n",
      "AMZN\n",
      "ATVI\n",
      "AVGO\n",
      "BIDU\n",
      "BIIB\n",
      "BMRN\n",
      "CA\n",
      "CELG\n",
      "CERN\n",
      "CHTR\n",
      "CMCSA\n",
      "COST\n",
      "CSCO\n",
      "CSX\n",
      "CTAS\n",
      "CTRP\n",
      "CTSH\n",
      "CTXS\n",
      "DISCA\n",
      "DISH\n",
      "DLTR\n",
      "EA\n",
      "EBAY\n",
      "ESRX\n",
      "EXPE\n",
      "FAST\n",
      "FB\n",
      "FISV\n",
      "FOXA\n",
      "GILD\n",
      "GOOGL\n",
      "HAS\n",
      "HOLX\n",
      "HSIC\n",
      "IDXX\n",
      "ILMN\n",
      "INCY\n",
      "INTC\n",
      "INTU\n",
      "ISRG\n",
      "JBHT\n",
      "JD\n",
      "KHC\n",
      "KLAC\n",
      "LBTYA\n",
      "LRCX\n",
      "MAR\n",
      "MAT\n",
      "MCHP\n",
      "MDLZ\n",
      "MELI\n",
      "MNST\n",
      "MSFT\n",
      "MU\n",
      "MXIM\n",
      "MYL\n",
      "NFLX\n",
      "NTES\n",
      "NVDA\n",
      "ORLY\n",
      "PAYX\n",
      "PCAR\n",
      "PCLN\n",
      "PYPL\n",
      "QCOM\n",
      "REGN\n",
      "ROST\n",
      "SBUX\n",
      "SHPG\n",
      "SIRI\n",
      "STX\n",
      "SWKS\n",
      "SYMC\n",
      "TMUS\n",
      "TSCO\n",
      "TSLA\n",
      "TXN\n",
      "ULTA\n",
      "VIAB\n",
      "VOD\n",
      "VRSK\n",
      "VRTX\n",
      "WBA\n",
      "WYNN\n",
      "XLNX\n",
      "XRAY\n"
     ]
    }
   ],
   "source": [
    "## -------- EXTRACT SENTIMENT SINGLE NEWS \n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(ticker)\n",
    "    news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDataset/preprocessing/preprocessed/'+ticker+'.csv')\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    # #news.head(10)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "\n",
    "\n",
    "    sentiment = pd.DataFrame(columns=['PUBLICATION_DATE','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY'])\n",
    "\n",
    "    for i, row in news.iterrows():\n",
    "        try:\n",
    "            sent = wordCountSentiment.getSentiment(news['SUMMARY'][i], dictionary)\n",
    "            sentiment.loc[i] = {'PUBLICATION_DATE':news['PUBLICATION_DATE'][i], 'CONSTRAINING':sent['CONSTRAINING'],\n",
    "                            'LITIGIOUS': sent['LITIGIOUS'],'NEGATIVE': sent['NEGATIVE'], 'POSITIVE': sent['POSITIVE'],\n",
    "                            'UNCERTAINTY': sent['UNCERTAINTY']}\n",
    "        except:\n",
    "            print(\"ERROR news \"+str(i))\n",
    "\n",
    "  \n",
    "    sentiment.to_csv('SentimentSingleNews/'+ticker+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  initTime\n",
      "0        1\n",
      "1        5\n"
     ]
    }
   ],
   "source": [
    "sentiment = pd.DataFrame(columns=['initTime'])\n",
    "sentiment.loc[0] = {'initTime':1 }\n",
    "sentiment.loc[1] = {'initTime':2 }\n",
    "print(sentiment['initTime'][0])\n",
    "sentiment['initTime'][1]=5\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers: \n",
    "     print(ticker)\n",
    "    news =  pd.read_csv('/home/andrea/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'+ticker+'.csv')\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    # #news.head(10)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "    \n",
    "    timeSpan = pd.read_csv(\"/home/andrea/Desktop/NLFF/TechnicalDatasetUpdated\"+ticker+\".csv\")\n",
    "    #timeSpan.rename(columns={'Unnamed: 0':'date'}, inplace= True)\n",
    "    timeSpan = timeSpan['date'].tolist()\n",
    "    # From GMT+8 to GMT+0\n",
    "    timeSpan = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in timeSpan]\n",
    "    timeSpan = [row-timedelta(hours=8) for row in timeSpan]\n",
    "\n",
    "\n",
    "    sentiment = pd.DataFrame(columns=['initTime','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING', 'NUM_NEWS'])\n",
    "\n",
    "\n",
    "    #Prendiamo intervallo massimo dove ci sono entrambi i dataset:\n",
    "    #massimo della data iniziale tra i due\n",
    "    #minimo della data finale tra i due\n",
    "    initDate = max(timeSpan[0], news['PUBLICATION_DATE'][0])\n",
    "    finalDate = min(timeSpan[len(timeSpan)-1], news['PUBLICATION_DATE'][len(news)-1])\n",
    "    last_news={'initTime':0,'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0}\n",
    "\n",
    "\n",
    "\n",
    "    i = 0;\n",
    "    j = 0\n",
    "    normal_sum = {'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0,'SUPERFLUOUS':0,'INTERESTING':0}\n",
    "\n",
    "    # ALLINEAMENTO INIZIO\n",
    "    while(timeSpan[j] < initDate):\n",
    "        j+=1\n",
    "    init = j\n",
    "    while(news['PUBLICATION_DATE'][i] <= initDate):\n",
    "        i+=1\n",
    "\n",
    "    print(\"Start: \"+str(initDate) +\" i: \"+str(i)+\" j: \"+str(j))\n",
    "    print(\"End: \"+str(finalDate))\n",
    "\n",
    "    #ALLINEAMENTO FINE DENTRO I WHILE\n",
    "    while( news['PUBLICATION_DATE'][i] < finalDate and timeSpan[j] < finalDate ):\n",
    "        num_sentiment = 0\n",
    "        initTime = timeSpan[j]    \n",
    "        while(i<len(news)-1 and timeSpan[j] > news['PUBLICATION_DATE'][i]):\n",
    "            if not (timeSpan[j] > news['PUBLICATION_DATE'][i] and timeSpan[j-1] <= news['PUBLICATION_DATE'][i]):\n",
    "                print(\"timeSpan[\"+str(j)+\"]: \"+str(timeSpan[j])+\" news[\"+str(i)+\"] : \" +str(news['PUBLICATION_DATE'][i]) + \" timeSpan[\"+str(j-1)+\"]: \"+str(timeSpan[j-1]))\n",
    "                assert False\n",
    "            try:\n",
    "                sent = wordCountSentiment.getSentiment(news['SUMMARY'][i], dictionary)\n",
    "                normal_sum['CONSTRAINING'] += sent['CONSTRAINING']\n",
    "                normal_sum['LITIGIOUS'] += sent['LITIGIOUS']\n",
    "                normal_sum['NEGATIVE'] += sent['NEGATIVE']\n",
    "                normal_sum['POSITIVE'] += sent['POSITIVE']\n",
    "                normal_sum['UNCERTAINTY'] += sent['UNCERTAINTY']\n",
    "                normal_sum['SUPERFLUOUS'] += sent['SUPERFLUOUS']\n",
    "                normal_sum['INTERESTING'] += sent['INTERESTING']\n",
    "                num_sentiment +=1\n",
    "            except:\n",
    "                print(\"ERROR news \"+str(i))\n",
    "            i+=1\n",
    "        for key, value in normal_sum.items():\n",
    "            if(num_sentiment != 0):\n",
    "                normal_sum[key] /=num_sentiment\n",
    "        j+=1\n",
    "        sentiment.loc[j] = {'initTime':initTime, 'CONSTRAINING':normal_sum['CONSTRAINING'],\n",
    "                            'LITIGIOUS': normal_sum['LITIGIOUS'],\n",
    "                            'NEGATIVE': normal_sum['NEGATIVE'], 'POSITIVE': normal_sum['POSITIVE'],\n",
    "                            'UNCERTAINTY': normal_sum['UNCERTAINTY'], 'SUPERFLUOUS': normal_sum['SUPERFLUOUS'],\n",
    "                            'INTERESTING': normal_sum['INTERESTING'], 'NUM_NEWS':num_sentiment }\n",
    "    \n",
    "    \n",
    "    \n",
    "    end = j\n",
    "    if(len(sentiment) != end - init):\n",
    "        print(len(sentiment))\n",
    "        print(end - init + 1)\n",
    "        assert False\n",
    "\n",
    "    \n",
    "    sentiment.to_csv('/home/andrea/Desktop/NLFF/intrinioDatasetUpdated/Sentiment'+ticker+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#------------ SENTIMENT OF ALL APPL\n",
    "\n",
    "news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDataset/preprocessing/preprocessed/AAPL.csv')\n",
    "tickers = ['AAPL2','AAPL3','AAPL4']\n",
    "for ticker in tickers:\n",
    "    newsTemp = pd.read_csv('/home/simone/Desktop/NLFF/intrinioDataset/preprocessing/preprocessed/' + ticker + '.csv')\n",
    "    news = pd.concat([news, newsTemp])\n",
    "news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "news = news.reset_index(drop=True)\n",
    "news = news.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "print(len(news))\n",
    "notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "NaN = []\n",
    "for i in news.index:\n",
    "    if(i not in notNaN):\n",
    "        NaN.append(i)\n",
    "news = news.drop(NaN)\n",
    "print(len(news))\n",
    "# #news.head(10)\n",
    "news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "XRAY\n"
     ]
    }
   ],
   "source": [
    "## -------- EXTRACT SENTIMENT SINGLE NEWS \n",
    "\n",
    "for ticker in tickers:\n",
    "    print(ticker)\n",
    "    news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDataset/preprocessing/preprocessed/'+ticker+'.csv')\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    # #news.head(10)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "\n",
    "\n",
    "    sentiment = pd.DataFrame(columns=['PUBLICATION_DATE','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING'])\n",
    "\n",
    "    for i, row in news.iterrows():\n",
    "        try:\n",
    "            sent = wordCountSentiment.getSentiment(news['SUMMARY'][i], dictionary)\n",
    "            sentiment.loc[i] = {'PUBLICATION_DATE':news['PUBLICATION_DATE'][i], 'CONSTRAINING':sent['CONSTRAINING'],\n",
    "                            'LITIGIOUS': sent['LITIGIOUS'],'NEGATIVE': sent['NEGATIVE'], 'POSITIVE': sent['POSITIVE'],\n",
    "                            'UNCERTAINTY': sent['UNCERTAINTY'],'SUPERFLUOUS': sent['SUPERFLUOUS'],'INTERESTING': sent['INTERESTING']}\n",
    "        except:\n",
    "            print(\"ERROR news \"+str(i))\n",
    "\n",
    "  \n",
    "    sentiment.to_csv('SentimentSingleNews/'+ticker+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  initTime\n",
      "0        1\n",
      "1        5\n"
     ]
    }
   ],
   "source": [
    "sentiment = pd.DataFrame(columns=['initTime'])\n",
    "sentiment.loc[0] = {'initTime':1 }\n",
    "sentiment.loc[1] = {'initTime':2 }\n",
    "print(sentiment['initTime'][0])\n",
    "sentiment['initTime'][1]=5\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
