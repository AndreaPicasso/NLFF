{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/simone/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordCountSentimentFull as wordCountSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEGATIVE</th>\n",
       "      <th>POSITIVE</th>\n",
       "      <th>UNCERTAINTY</th>\n",
       "      <th>LITIGIOUS</th>\n",
       "      <th>CONSTRAINING</th>\n",
       "      <th>SUPERFLUOUS</th>\n",
       "      <th>INTERESTING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>ABEYANCE</td>\n",
       "      <td>ABOVEMENTIONED</td>\n",
       "      <td>ABIDE</td>\n",
       "      <td>AEGIS</td>\n",
       "      <td>AGGRESSIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDONED</td>\n",
       "      <td>ABUNDANCE</td>\n",
       "      <td>ABEYANCES</td>\n",
       "      <td>ABROGATE</td>\n",
       "      <td>ABIDING</td>\n",
       "      <td>AMORPHOUS</td>\n",
       "      <td>AGGRESSIVELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONING</td>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>ALMOST</td>\n",
       "      <td>ABROGATED</td>\n",
       "      <td>BOUND</td>\n",
       "      <td>ANTICIPATORY</td>\n",
       "      <td>AGGRESSIVENESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>ACCLAIMED</td>\n",
       "      <td>ALTERATION</td>\n",
       "      <td>ABROGATES</td>\n",
       "      <td>BOUNDED</td>\n",
       "      <td>APPERTAINING</td>\n",
       "      <td>ASBESTOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "      <td>ACCOMPLISH</td>\n",
       "      <td>ALTERATIONS</td>\n",
       "      <td>ABROGATING</td>\n",
       "      <td>COMMIT</td>\n",
       "      <td>ASSIMILATE</td>\n",
       "      <td>AUGUST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NEGATIVE    POSITIVE  UNCERTAINTY       LITIGIOUS CONSTRAINING  \\\n",
       "0       ABANDON        ABLE     ABEYANCE  ABOVEMENTIONED        ABIDE   \n",
       "1     ABANDONED   ABUNDANCE    ABEYANCES        ABROGATE      ABIDING   \n",
       "2    ABANDONING    ABUNDANT       ALMOST       ABROGATED        BOUND   \n",
       "3   ABANDONMENT   ACCLAIMED   ALTERATION       ABROGATES      BOUNDED   \n",
       "4  ABANDONMENTS  ACCOMPLISH  ALTERATIONS      ABROGATING       COMMIT   \n",
       "\n",
       "    SUPERFLUOUS     INTERESTING  \n",
       "0         AEGIS      AGGRESSIVE  \n",
       "1     AMORPHOUS    AGGRESSIVELY  \n",
       "2  ANTICIPATORY  AGGRESSIVENESS  \n",
       "3  APPERTAINING        ASBESTOS  \n",
       "4    ASSIMILATE          AUGUST  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = pd.read_csv('LoughranMcDonald_MyDictionary2.csv')\n",
    "dictionary.drop('Unnamed: 0', 1, inplace=True)\n",
    "\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEGATIVE</th>\n",
       "      <th>POSITIVE</th>\n",
       "      <th>UNCERTAINTY</th>\n",
       "      <th>LITIGIOUS</th>\n",
       "      <th>CONSTRAINING</th>\n",
       "      <th>SUPERFLUOUS</th>\n",
       "      <th>INTERESTING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abl</td>\n",
       "      <td>abey</td>\n",
       "      <td>abovement</td>\n",
       "      <td>abid</td>\n",
       "      <td>aegi</td>\n",
       "      <td>aggress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abund</td>\n",
       "      <td>abey</td>\n",
       "      <td>abrog</td>\n",
       "      <td>abid</td>\n",
       "      <td>amorph</td>\n",
       "      <td>aggress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abund</td>\n",
       "      <td>almost</td>\n",
       "      <td>abrog</td>\n",
       "      <td>bound</td>\n",
       "      <td>anticipatori</td>\n",
       "      <td>aggress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>acclaim</td>\n",
       "      <td>alter</td>\n",
       "      <td>abrog</td>\n",
       "      <td>bound</td>\n",
       "      <td>appertain</td>\n",
       "      <td>asbesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandon</td>\n",
       "      <td>accomplish</td>\n",
       "      <td>alter</td>\n",
       "      <td>abrog</td>\n",
       "      <td>commit</td>\n",
       "      <td>assimil</td>\n",
       "      <td>august</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  NEGATIVE    POSITIVE UNCERTAINTY  LITIGIOUS CONSTRAINING   SUPERFLUOUS  \\\n",
       "0  abandon         abl        abey  abovement         abid          aegi   \n",
       "1  abandon       abund        abey      abrog         abid        amorph   \n",
       "2  abandon       abund      almost      abrog        bound  anticipatori   \n",
       "3  abandon     acclaim       alter      abrog        bound     appertain   \n",
       "4  abandon  accomplish       alter      abrog       commit       assimil   \n",
       "\n",
       "  INTERESTING  \n",
       "0     aggress  \n",
       "1     aggress  \n",
       "2     aggress  \n",
       "3     asbesto  \n",
       "4      august  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountSentiment.lemmatizeDictionary(dictionary)\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEGATIVE': 0,\n",
       " 'POSITIVE': 1,\n",
       " 'UNCERTAINTY': 1,\n",
       " 'LITIGIOUS': 1,\n",
       " 'CONSTRAINING': 1,\n",
       " 'SUPERFLUOUS': 0,\n",
       " 'INTERESTING': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountSentiment.getSentiment('Apple is designing its own power-management chips for use in iPhones as early as 2018, the Nikkei business daily reported on Thursday, triggering a more than 20 percent slide in shares of supplier Dialog Semiconductor.  If confirmed, the move would reduce Apple is dependence on the Anglo-German chipmaker, which itself is heavily reliant on the smartphone industry and has been trying to diversify its customer base.  Investors are particularly jittery after Apple said in April that it planned to replace graphics chip supplier Imagination Technologies, sending the London-listed stock down 70 percent in a single session.',dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tickers=['AAPL','AMZN','GOOGL','MSFT','FB','INTC','CSCO','CMCSA','NVDA','NFLX','ADBE','AMGN','TXN','AVGO','PYPL','GILD','COST','QCOM']       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------- EXTRACT SENTIMENT SINGLE NEWS \n",
    "\n",
    "mypath = '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'\n",
    "tickFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for ticker in tickFiles:\n",
    "    print(ticker)\n",
    "    news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'+ticker)\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    # #news.head(10)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "\n",
    "\n",
    "    sentiment = pd.DataFrame(columns=['PUBLICATION_DATE','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING'])\n",
    "\n",
    "    for i, row in news.iterrows():\n",
    "        try:\n",
    "            sent = wordCountSentiment.getSentiment(news['SUMMARY'][i], dictionary)\n",
    "            sentiment.loc[i] = {'PUBLICATION_DATE':news['PUBLICATION_DATE'][i], 'CONSTRAINING':sent['CONSTRAINING'],\n",
    "                            'LITIGIOUS': sent['LITIGIOUS'],'NEGATIVE': sent['NEGATIVE'], 'POSITIVE': sent['POSITIVE'],\n",
    "                            'UNCERTAINTY': sent['UNCERTAINTY'],'SUPERFLUOUS': sent['SUPERFLUOUS'],'INTERESTING': sent['INTERESTING']}\n",
    "        except:\n",
    "            print(\"ERROR news \"+str(i))\n",
    "\n",
    "  \n",
    "    sentiment.to_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/Sentiment/'+ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT SENTIMENT AGGREGATED BY TIMESPAN\n",
    "\n",
    "mypath = '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'\n",
    "tickFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for ticker in tickers: \n",
    "    print(ticker)\n",
    "    news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'+ticker+'.csv')\n",
    "    count =1\n",
    "    while str(ticker)+str(count)+'.csv' in tickFiles:\n",
    "        print(ticker+str(count))\n",
    "        newsTemp = pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/' + ticker +str(count)+'.csv')\n",
    "        news = pd.concat([news, newsTemp])\n",
    "        count+=1\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    #news.head(10)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "    \n",
    "    timeSpan = pd.read_csv(\"/home/simone/Desktop/NLFF/indexes/indexes\"+ticker+\".csv\")\n",
    "    #timeSpan.rename(columns={'Unnamed: 0':'date'}, inplace= True)\n",
    "    timeSpan = timeSpan['date'].tolist()\n",
    "    # This dataset is already GMT+0\n",
    "    timeSpan = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in timeSpan]\n",
    "    #timeSpan = [row-timedelta(hours=8) for row in timeSpan]\n",
    "\n",
    "\n",
    "    sentiment = pd.DataFrame(columns=['initTime','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING', 'NUM_NEWS'])\n",
    "\n",
    "\n",
    "    #Prendiamo intervallo massimo dove ci sono entrambi i dataset:\n",
    "    #massimo della data iniziale tra i due\n",
    "    #minimo della data finale tra i due\n",
    "    initDate = max(timeSpan[0], news['PUBLICATION_DATE'][0])\n",
    "    finalDate = min(timeSpan[len(timeSpan)-1], news['PUBLICATION_DATE'][len(news)-1])\n",
    "    last_news={'initTime':0,'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0}\n",
    "\n",
    "\n",
    "\n",
    "    i = 0;\n",
    "    j = 0\n",
    "\n",
    "    # ALLINEAMENTO INIZIO\n",
    "    while(timeSpan[j] < initDate):\n",
    "        j+=1\n",
    "    init = j\n",
    "    while(news['PUBLICATION_DATE'][i] <= initDate):\n",
    "        i+=1\n",
    "\n",
    "    print(\"Start: \"+str(initDate) +\" i: \"+str(i)+\" j: \"+str(j))\n",
    "    print(\"End: \"+str(finalDate))\n",
    "\n",
    "    #ALLINEAMENTO FINE DENTRO I WHILE\n",
    "    while( news['PUBLICATION_DATE'][i] < finalDate and timeSpan[j] < finalDate ):\n",
    "        num_sentiment = 0\n",
    "        initTime = timeSpan[j]\n",
    "        normal_sum = {'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0,'SUPERFLUOUS':0,'INTERESTING':0}\n",
    "        while(i<len(news)-1 and timeSpan[j] > news['PUBLICATION_DATE'][i]):\n",
    "            if not (timeSpan[j] > news['PUBLICATION_DATE'][i] and timeSpan[j-1] <= news['PUBLICATION_DATE'][i]):\n",
    "                print(\"timeSpan[\"+str(j)+\"]: \"+str(timeSpan[j])+\" news[\"+str(i)+\"] : \" +str(news['PUBLICATION_DATE'][i]) + \" timeSpan[\"+str(j-1)+\"]: \"+str(timeSpan[j-1]))\n",
    "                assert False\n",
    "            try:\n",
    "                sent = wordCountSentiment.getSentiment(news['SUMMARY'][i], dictionary)\n",
    "                normal_sum['CONSTRAINING'] += sent['CONSTRAINING']\n",
    "                normal_sum['LITIGIOUS'] += sent['LITIGIOUS']\n",
    "                normal_sum['NEGATIVE'] += sent['NEGATIVE']\n",
    "                normal_sum['POSITIVE'] += sent['POSITIVE']\n",
    "                normal_sum['UNCERTAINTY'] += sent['UNCERTAINTY']\n",
    "                normal_sum['SUPERFLUOUS'] += sent['SUPERFLUOUS']\n",
    "                normal_sum['INTERESTING'] += sent['INTERESTING']\n",
    "                num_sentiment +=1\n",
    "            except:\n",
    "                print(\"ERROR news \"+str(i))\n",
    "            i+=1\n",
    "        for key, value in normal_sum.items():\n",
    "            if(num_sentiment != 0):\n",
    "                normal_sum[key] /=num_sentiment\n",
    "        j+=1\n",
    "        sentiment.loc[j] = {'initTime':initTime, 'CONSTRAINING':normal_sum['CONSTRAINING'],\n",
    "                            'LITIGIOUS': normal_sum['LITIGIOUS'],\n",
    "                            'NEGATIVE': normal_sum['NEGATIVE'], 'POSITIVE': normal_sum['POSITIVE'],\n",
    "                            'UNCERTAINTY': normal_sum['UNCERTAINTY'], 'SUPERFLUOUS': normal_sum['SUPERFLUOUS'],\n",
    "                            'INTERESTING': normal_sum['INTERESTING'], 'NUM_NEWS':num_sentiment }\n",
    "    \n",
    "    \n",
    "    \n",
    "    end = j\n",
    "    if(len(sentiment) != end - init):\n",
    "        print(len(sentiment))\n",
    "        print(end - init + 1)\n",
    "        assert False\n",
    "\n",
    "    \n",
    "    sentiment.to_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentFullAggregatedHourly/'+ticker+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "AAPL1\n",
      "AAPL2\n",
      "Start: 2017-04-03 17:27:34 i: 1 j: 3817\n",
      "End: 2018-06-21 23:20:00\n",
      "AMZN\n",
      "AMZN1\n",
      "AMZN2\n",
      "Start: 2017-04-26 13:00:00 i: 1 j: 3925\n",
      "End: 2018-06-22 02:35:18\n",
      "GOOGL\n",
      "GOOGL1\n",
      "Start: 2017-05-21 15:00:05 i: 1 j: 4051\n",
      "End: 2018-06-22 00:14:21\n",
      "MSFT\n",
      "MSFT1\n",
      "Start: 2016-04-22 01:05:08 i: 1 j: 2150\n",
      "End: 2018-06-22 00:10:12\n",
      "FB\n",
      "FB1\n",
      "FB2\n",
      "Start: 2016-04-28 00:17:37 i: 1 j: 2178\n",
      "End: 2018-06-21 23:30:00\n",
      "INTC\n",
      "Start: 2017-02-15 14:20:32 i: 1 j: 3589\n",
      "End: 2018-06-22 02:45:56\n",
      "CSCO\n",
      "Start: 2015-02-11 22:25:31 i: 1 j: 56\n",
      "End: 2018-06-20 19:12:55\n",
      "CMCSA\n",
      "Start: 2015-04-21 17:28:40 i: 1 j: 382\n",
      "End: 2018-06-21 22:32:37\n",
      "NVDA\n",
      "Start: 2017-01-19 22:49:53 i: 1 j: 3463\n",
      "End: 2018-06-21 22:16:10\n",
      "NFLX\n",
      "NFLX1\n",
      "Start: 2016-01-05 13:01:49 i: 1 j: 1625\n",
      "End: 2018-06-21 22:54:33\n",
      "ADBE\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/simone/Desktop/NLFF/indexes/indexesADBE.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-909d6cf8e2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtimeSpan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/simone/Desktop/NLFF/indexes/indexes\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m#timeSpan.rename(columns={'Unnamed: 0':'date'}, inplace= True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtimeSpan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeSpan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/simone/Desktop/NLFF/indexes/indexesADBE.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# EXTRACT SENTIMENT OF TITLE AGGREGATED BY TIMESPAN\n",
    "\n",
    "mypath = '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'\n",
    "tickFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for ticker in tickers: \n",
    "    print(ticker)\n",
    "    news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'+ticker+'.csv')\n",
    "    count =1\n",
    "    while str(ticker)+str(count)+'.csv' in tickFiles:\n",
    "        print(ticker+str(count))\n",
    "        newsTemp = pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/' + ticker +str(count)+'.csv')\n",
    "        news = pd.concat([news, newsTemp])\n",
    "        count+=1\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['TITLE'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    #news.head(10)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "    \n",
    "    timeSpan = pd.read_csv(\"/home/simone/Desktop/NLFF/indexes/indexes\"+ticker+\".csv\")\n",
    "    #timeSpan.rename(columns={'Unnamed: 0':'date'}, inplace= True)\n",
    "    timeSpan = timeSpan['date'].tolist()\n",
    "    # This dataset is already GMT+0\n",
    "    timeSpan = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in timeSpan]\n",
    "    #timeSpan = [row-timedelta(hours=8) for row in timeSpan]\n",
    "\n",
    "\n",
    "    sentiment = pd.DataFrame(columns=['initTime','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING', 'NUM_NEWS'])\n",
    "\n",
    "\n",
    "    #Prendiamo intervallo massimo dove ci sono entrambi i dataset:\n",
    "    #massimo della data iniziale tra i due\n",
    "    #minimo della data finale tra i due\n",
    "    initDate = max(timeSpan[0], news['PUBLICATION_DATE'][0])\n",
    "    finalDate = min(timeSpan[len(timeSpan)-1], news['PUBLICATION_DATE'][len(news)-1])\n",
    "    last_news={'initTime':0,'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0}\n",
    "\n",
    "\n",
    "\n",
    "    i = 0;\n",
    "    j = 0\n",
    "\n",
    "    # ALLINEAMENTO INIZIO\n",
    "    while(timeSpan[j] < initDate):\n",
    "        j+=1\n",
    "    init = j\n",
    "    while(news['PUBLICATION_DATE'][i] <= initDate):\n",
    "        i+=1\n",
    "\n",
    "    print(\"Start: \"+str(initDate) +\" i: \"+str(i)+\" j: \"+str(j))\n",
    "    print(\"End: \"+str(finalDate))\n",
    "\n",
    "    #ALLINEAMENTO FINE DENTRO I WHILE\n",
    "    while( news['PUBLICATION_DATE'][i] < finalDate and timeSpan[j] < finalDate ):\n",
    "        num_sentiment = 0\n",
    "        initTime = timeSpan[j]\n",
    "        normal_sum = {'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0,'SUPERFLUOUS':0,'INTERESTING':0}\n",
    "        while(i<len(news)-1 and timeSpan[j] > news['PUBLICATION_DATE'][i]):\n",
    "            if not (timeSpan[j] > news['PUBLICATION_DATE'][i] and timeSpan[j-1] <= news['PUBLICATION_DATE'][i]):\n",
    "                print(\"timeSpan[\"+str(j)+\"]: \"+str(timeSpan[j])+\" news[\"+str(i)+\"] : \" +str(news['PUBLICATION_DATE'][i]) + \" timeSpan[\"+str(j-1)+\"]: \"+str(timeSpan[j-1]))\n",
    "                assert False\n",
    "            try:\n",
    "                sent = wordCountSentiment.getSentiment(news['TITLE'][i], dictionary)\n",
    "                normal_sum['CONSTRAINING'] += sent['CONSTRAINING']\n",
    "                normal_sum['LITIGIOUS'] += sent['LITIGIOUS']\n",
    "                normal_sum['NEGATIVE'] += sent['NEGATIVE']\n",
    "                normal_sum['POSITIVE'] += sent['POSITIVE']\n",
    "                normal_sum['UNCERTAINTY'] += sent['UNCERTAINTY']\n",
    "                normal_sum['SUPERFLUOUS'] += sent['SUPERFLUOUS']\n",
    "                normal_sum['INTERESTING'] += sent['INTERESTING']\n",
    "                num_sentiment +=1\n",
    "            except:\n",
    "                print(\"ERROR news \"+str(i))\n",
    "            i+=1\n",
    "        for key, value in normal_sum.items():\n",
    "            if(num_sentiment != 0):\n",
    "                normal_sum[key] /=num_sentiment\n",
    "        j+=1\n",
    "        sentiment.loc[j] = {'initTime':initTime, 'CONSTRAINING':normal_sum['CONSTRAINING'],\n",
    "                            'LITIGIOUS': normal_sum['LITIGIOUS'],\n",
    "                            'NEGATIVE': normal_sum['NEGATIVE'], 'POSITIVE': normal_sum['POSITIVE'],\n",
    "                            'UNCERTAINTY': normal_sum['UNCERTAINTY'], 'SUPERFLUOUS': normal_sum['SUPERFLUOUS'],\n",
    "                            'INTERESTING': normal_sum['INTERESTING'], 'NUM_NEWS':num_sentiment }\n",
    "    \n",
    "    \n",
    "    \n",
    "    end = j\n",
    "    if(len(sentiment) != end - init):\n",
    "        print(len(sentiment))\n",
    "        print(end - init + 1)\n",
    "        assert False\n",
    "\n",
    "    \n",
    "    sentiment.to_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentTitleAggregatedHourly/'+ticker+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
