{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "\n",
    "import wordCountSentimentFull as wordCountSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_csv('LoughranMcDonald_MyDictionary2.csv')\n",
    "dictionary.drop('Unnamed: 0', 1, inplace=True)\n",
    "dictionary = dictionary.fillna('')\n",
    "wordCountSentiment.lemmatizeDictionary(dictionary)\n",
    "allWords = list()\n",
    "word2category = {}\n",
    "for col in ['CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING']:\n",
    "    allWords = allWords + dictionary[col].tolist()\n",
    "    for w in dictionary[col]:\n",
    "        word2category[w] = col\n",
    "allWords.remove('')\n",
    "allWords = list(set(allWords))\n",
    "\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#     tokens = word_tokenize(sentence)\n",
    "#     sentence = [porter_stemmer.stem(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=['AAPL','AMZN','GOOGL','MSFT','FB','INTC','CSCO','CMCSA','NVDA','NFLX']\n",
    "#tickers=['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "AAPL1\n",
      "AAPL2\n",
      "Start: 2017-04-03 17:27:34 i: 1 j: 3817\n",
      "End: 2018-06-21 23:20:00\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "AMZN\n",
      "AMZN1\n",
      "AMZN2\n",
      "Start: 2017-04-26 13:00:00 i: 1 j: 3925\n",
      "End: 2018-06-22 02:35:18\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "GOOGL\n",
      "GOOGL1\n",
      "Start: 2017-05-21 15:00:05 i: 1 j: 4051\n",
      "End: 2018-06-22 00:14:21\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "MSFT\n",
      "MSFT1\n",
      "Start: 2016-04-22 01:05:08 i: 1 j: 2150\n",
      "End: 2018-06-22 00:10:12\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "FB\n",
      "FB1\n",
      "FB2\n",
      "Start: 2016-04-28 00:17:37 i: 1 j: 2178\n",
      "End: 2018-06-21 23:30:00\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "INTC\n",
      "Start: 2017-02-15 14:20:32 i: 1 j: 3589\n",
      "End: 2018-06-22 02:45:56\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "CSCO\n",
      "Start: 2015-02-11 22:25:31 i: 1 j: 56\n",
      "End: 2018-06-20 19:12:55\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "CMCSA\n",
      "Start: 2015-04-21 17:28:40 i: 1 j: 382\n",
      "End: 2018-06-21 22:32:37\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "NVDA\n",
      "Start: 2017-01-19 22:49:53 i: 1 j: 3463\n",
      "End: 2018-06-21 22:16:10\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "NFLX\n",
      "NFLX1\n",
      "Start: 2016-01-05 13:01:49 i: 1 j: 1625\n",
      "End: 2018-06-21 22:54:33\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT SENTIMENT AGGREGATED BY TIMESPAN\n",
    "\n",
    "mypath = '/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'\n",
    "tickFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for ticker in tickers: \n",
    "    print(ticker)\n",
    "    news =  pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/'+ticker+'.csv')\n",
    "    count =1\n",
    "    while str(ticker)+str(count)+'.csv' in tickFiles:\n",
    "        print(ticker+str(count))\n",
    "        newsTemp = pd.read_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/preprocessing/preprocessed/' + ticker +str(count)+'.csv')\n",
    "        news = pd.concat([news, newsTemp])\n",
    "        count+=1\n",
    "    news.drop_duplicates(subset=['PUBLICATION_DATE'], inplace=True)\n",
    "    news = news.sort_values(by=['PUBLICATION_DATE'])\n",
    "    news = news.reset_index(drop=True)\n",
    "    news = news.drop(['Unnamed: 0'], axis=1)\n",
    "    notNaN = news['SUMMARY'].dropna(how='any').index.tolist()\n",
    "    NaN = []\n",
    "    for i in news.index:\n",
    "        if(i not in notNaN):\n",
    "            NaN.append(i)\n",
    "    news = news.drop(NaN)\n",
    "    news = news.reset_index(drop=True)\n",
    "    news['PUBLICATION_DATE'] = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S +%f') for row in news['PUBLICATION_DATE']]\n",
    "    \n",
    "    \n",
    "    timeSpan = pd.read_csv(\"/home/simone/Desktop/NLFF/indexes/indexes\"+ticker+\".csv\")\n",
    "    timeSpan = timeSpan['date'].tolist()\n",
    "    # This dataset is already GMT+0\n",
    "    timeSpan = [datetime.strptime(row, '%Y-%m-%d %H:%M:%S') for row in timeSpan]\n",
    "    sentiment = pd.DataFrame(columns=['initTime','num_news']+allWords)\n",
    "    initDate = max(timeSpan[0], news['PUBLICATION_DATE'][0])\n",
    "    finalDate = min(timeSpan[len(timeSpan)-1], news['PUBLICATION_DATE'][len(news)-1])\n",
    "    i = 0;\n",
    "    j = 0\n",
    "    # ALLINEAMENTO INIZIO\n",
    "    while(timeSpan[j] < initDate):\n",
    "        j+=1\n",
    "    init = j\n",
    "    while(news['PUBLICATION_DATE'][i] <= initDate):\n",
    "        i+=1\n",
    "    print(\"Start: \"+str(initDate) +\" i: \"+str(i)+\" j: \"+str(j))\n",
    "    print(\"End: \"+str(finalDate))\n",
    "    #ALLINEAMENTO FINE DENTRO I WHILE\n",
    "    while( news['PUBLICATION_DATE'][i] < finalDate and timeSpan[j] < finalDate ):\n",
    "        num_sentiment = 0\n",
    "        initTime = timeSpan[j]\n",
    "        normal_sum = {w:0 for w in allWords+['initTime','num_news']}\n",
    "        num_sentiment = {w:0 for w in allWords}\n",
    "        num_news = 0\n",
    "        while(i<len(news)-1 and timeSpan[j] > news['PUBLICATION_DATE'][i]):\n",
    "            if not (timeSpan[j] > news['PUBLICATION_DATE'][i] and timeSpan[j-1] <= news['PUBLICATION_DATE'][i]):\n",
    "                print(\"timeSpan[\"+str(j)+\"]: \"+str(timeSpan[j])+\" news[\"+str(i)+\"] : \" +str(news['PUBLICATION_DATE'][i]) + \" timeSpan[\"+str(j-1)+\"]: \"+str(timeSpan[j-1]))\n",
    "                assert False\n",
    "            try:\n",
    "                tokens = word_tokenize(news['SUMMARY'][i])\n",
    "                tokens = [porter_stemmer.stem(x) for x in tokens]\n",
    "                for token in tokens:\n",
    "                    if token in allWords:\n",
    "                        normal_sum[token] +=1\n",
    "                        num_sentiment[token] +=1\n",
    "                num_news += 1\n",
    "            except:\n",
    "                print(\"ERROR news \"+str(i))\n",
    "            i+=1\n",
    "            if(i%500 == 0):\n",
    "                print(i)\n",
    "        j+=1\n",
    "        normal_sum = {f:normal_sum[f]/num_sentiment[f] if num_sentiment[f]>0 else 0 for f in allWords}\n",
    "        normal_sum['initTime'] = initTime\n",
    "        normal_sum['num_news'] = num_news\n",
    "        sentiment.loc[j] = normal_sum\n",
    "    \n",
    "    #Normalize for max num words\n",
    "    M = sentiment.max()[2:]\n",
    "    for k in M.keys():\n",
    "        M[k] += 1 if( M[k]==0) else 0\n",
    "        sentiment[k] = sentiment[k]/M[k]\n",
    "    \n",
    "    #Multipy for factor of frequency of word\n",
    "    num_intervals = {w:0 for w in allWords}\n",
    "    for index, row in sentiment.iterrows():\n",
    "        for k in num_intervals.keys():\n",
    "            num_intervals[k] += 1 if sentiment[k].at[index]>0 else 0\n",
    "    for k in num_intervals.keys():\n",
    "        num_intervals[k] += 1 if num_intervals[k]==0 else 0\n",
    "        sentiment[k] = sentiment[k]*np.log(len(sentiment)/num_intervals[k])\n",
    "    #Compute mean \"TF-IDF\" frequency for all the words in a category\n",
    "    sentiment2 = pd.DataFrame(columns=['initTime','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING', 'NUM_NEWS'])\n",
    "    for i, row in sentiment.iterrows():\n",
    "        normal_sum = {'CONSTRAINING': 0, 'LITIGIOUS': 0, 'NEGATIVE': 0, 'POSITIVE': 0, 'UNCERTAINTY': 0,'SUPERFLUOUS':0,'INTERESTING':0}\n",
    "        for k in allWords:\n",
    "            normal_sum[word2category[k]] += row[k]\n",
    "        for col in ['CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING']:\n",
    "            normal_sum[col] = normal_sum[col]/ row['num_news'] if row['num_news']>0 else normal_sum[col]\n",
    "        sentiment2.loc[i] = {'initTime':row['initTime'], 'CONSTRAINING':normal_sum['CONSTRAINING'],\n",
    "                            'LITIGIOUS': normal_sum['LITIGIOUS'],\n",
    "                            'NEGATIVE': normal_sum['NEGATIVE'], 'POSITIVE': normal_sum['POSITIVE'],\n",
    "                            'UNCERTAINTY': normal_sum['UNCERTAINTY'], 'SUPERFLUOUS': normal_sum['SUPERFLUOUS'],\n",
    "                            'INTERESTING': normal_sum['INTERESTING'], 'NUM_NEWS':row['num_news'] }\n",
    "            \n",
    "    sentiment2.to_csv('/home/simone/Desktop/NLFF/intrinioDatasetUpdated/SentimentTF-IDF/'+ticker+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = sentiment.max()\n",
    "for k in M.keys():\n",
    "    M[k] += 1 if( M[k]==0) else 0\n",
    "    sentiment[k] = sentiment[k]/M[k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.DataFrame(columns=['initTime','CONSTRAINING','LITIGIOUS','NEGATIVE','POSITIVE','UNCERTAINTY','SUPERFLUOUS','INTERESTING', 'NUM_NEWS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
